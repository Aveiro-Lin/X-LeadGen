{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8633cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for\n",
      "       usage information.\n",
      "\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for\n",
      "       usage information.\n",
      "\n",
      "\u001b[1m\u001b[37mubuntu                    \u001b[m  Thu Feb  1 19:03:26 2024  \u001b[1m\u001b[30m510.47.03\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 36'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  311\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 37'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  311\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 45'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  311\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 35'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  311\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 38'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  311\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 57'C\u001b[m, \u001b[32m 13 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14328\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30mrobotics\u001b[m(\u001b[33m14017M\u001b[m)\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 52'C\u001b[m, \u001b[32m 14 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14328\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30mrobotics\u001b[m(\u001b[33m14017M\u001b[m)\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 56'C\u001b[m, \u001b[32m 14 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14328\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30mrobotics\u001b[m(\u001b[33m14017M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "# 这个进行ECG_unet的训练\n",
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bffb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用显卡定义，库函数的引用\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] ='2'\n",
    "device='cuda:0'\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import sys\n",
    "from datasets import train_loader\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.init as init\n",
    "from models.DG_models import generate_model\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3e4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def weights_init(init_type='gaussian'):\n",
    "    def init_fun(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if (classname.find('Conv') == 0 or classname.find('Linear') == 0) and hasattr(m, 'weight'):\n",
    "            # print m.__class__.__name__\n",
    "            if init_type == 'gaussian':\n",
    "                init.normal_(m.weight.data, 0.0, 0.02)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=math.sqrt(2))\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=math.sqrt(2))\n",
    "            elif init_type == 'default':\n",
    "                pass\n",
    "            else:\n",
    "                assert 0, \"Unsupported initialization: {}\".format(init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    return init_fun\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def get_model_list(dirname, key):\n",
    "    if os.path.exists(dirname) is False:\n",
    "        return None\n",
    "    gen_models = [os.path.join(dirname, f) for f in os.listdir(dirname) if\n",
    "                  os.path.isfile(os.path.join(dirname, f)) and key in f and \".pt\" in f]\n",
    "    if gen_models is None:\n",
    "        return None\n",
    "    gen_models.sort()\n",
    "    last_model_name = gen_models[-1]\n",
    "    return last_model_name\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_scheduler(optimizer, iterations=-1):\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=100000,\n",
    "                                        gamma=0.5, last_epoch=iterations)\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, msg):\n",
    "        self.msg = msg\n",
    "        self.start_time = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        print(self.msg % (time.time() - self.start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "429a310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络定义，包括gen_a,gen_b,dis_a,dis_b\n",
    "\n",
    "class AdaptiveInstanceNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(AdaptiveInstanceNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # weight and bias are dynamically assigned\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        # just dummy buffers, not used\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.weight is not None and self.bias is not None, \"Please assign weight and bias before calling AdaIN!\"\n",
    "        b, c = x.size(0), x.size(1)\n",
    "        running_mean = self.running_mean.repeat(b)\n",
    "        running_var = self.running_var.repeat(b)\n",
    "\n",
    "        # Apply instance norm\n",
    "        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n",
    "        out = F.batch_norm(\n",
    "            x_reshaped, running_mean, running_var, self.weight, self.bias,\n",
    "            True, self.momentum, self.eps)\n",
    "\n",
    "        return out.view(b, c, *x.size()[2:])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + str(self.num_features) + ')'\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # dec\n",
    "        self.conv1=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn1=AdaptiveInstanceNorm2d(64)\n",
    "        self.ac1=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv2=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2=AdaptiveInstanceNorm2d(64)\n",
    "        self.ac2=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv3=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn3=AdaptiveInstanceNorm2d(64)\n",
    "        self.ac3=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv4=nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn4=AdaptiveInstanceNorm2d(32)\n",
    "        self.ac4=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv5=nn.Conv1d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn5=AdaptiveInstanceNorm2d(32)\n",
    "        self.ac5=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv6=nn.Conv1d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn6=AdaptiveInstanceNorm2d(16)\n",
    "        self.ac6=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv7=nn.Conv1d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn7=AdaptiveInstanceNorm2d(16)\n",
    "        self.ac7=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv8=nn.Conv1d(16, 8, kernel_size=3, padding=1)\n",
    "        self.bn8=AdaptiveInstanceNorm2d(8)\n",
    "        self.ac8=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv9=nn.Conv1d(8, 8, kernel_size=3, padding=1)\n",
    "        self.ac9=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv10=nn.Conv1d(8, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x=self.ac1(x+self.bn1(self.conv1(x)))\n",
    "        x=self.ac2(x+self.bn2(self.conv2(x)))\n",
    "        x=self.ac3(x+self.bn3(self.conv3(x)))\n",
    "        x=self.ac4(self.bn4(self.conv4(x)))\n",
    "        x=self.ac5(self.bn5(self.conv5(x)))\n",
    "        x=self.ac6(self.bn6(self.conv6(x)))\n",
    "        x=self.ac7(self.bn7(self.conv7(x)))\n",
    "        x=self.ac8(self.bn8(self.conv8(x)))\n",
    "        x=self.conv10(self.ac9(self.conv9(x)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ECG_gen(nn.Module):  \n",
    "    def __init__(self,):  \n",
    "        super(ECG_gen, self).__init__()  \n",
    "        \n",
    "        # cont_enc\n",
    "        \n",
    "        self.conv1=nn.Conv1d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1=nn.InstanceNorm1d(8)\n",
    "        self.ac1=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv2=nn.Conv1d(8, 8, kernel_size=3, padding=1)\n",
    "        self.bn2=nn.InstanceNorm1d(8)\n",
    "        self.ac2=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv3=nn.Conv1d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn3=nn.InstanceNorm1d(16)\n",
    "        self.ac3=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv4=nn.Conv1d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn4=nn.InstanceNorm1d(16)\n",
    "        self.ac4=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv5=nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn5=nn.InstanceNorm1d(32)\n",
    "        self.ac5=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv6=nn.Conv1d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn6=nn.InstanceNorm1d(32)\n",
    "        self.ac6=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv7=nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn7=nn.InstanceNorm1d(64)\n",
    "        self.ac7=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv8=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn8=nn.InstanceNorm1d(64)\n",
    "        self.ac8=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv9=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn9=nn.InstanceNorm1d(64)\n",
    "        self.ac9=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv10=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn10=nn.InstanceNorm1d(64)\n",
    "        self.ac10=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv11=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn11=nn.InstanceNorm1d(64)\n",
    "        self.ac11=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        # style_enc\n",
    "        \n",
    "        self.conv_sty_1=nn.Conv1d(1, 8, kernel_size=3, padding=1)\n",
    "        self.ac_sty_1=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_2=nn.Conv1d(8, 16, kernel_size=3, padding=1)\n",
    "        self.ac_sty_2=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_3=nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.ac_sty_3=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_4=nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.ac_sty_4=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_5=nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.ac_sty_5=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_6=nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.ac_sty_6=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.pool_sty=nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc_sty=nn.Conv1d(256, 8, kernel_size=1, padding=0)\n",
    "        # bs,8,1\n",
    "        \n",
    "        self.dec=Decoder()\n",
    "        \n",
    "        self.mlp_1=nn.Linear(8, 256, bias=True)\n",
    "        self.mlp_2=nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.mlp_3=nn.Linear(256, 256, bias=True)\n",
    "        self.mlp_4=nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.mlp_5=nn.Linear(256, self.get_num_adain_params(self.dec), bias=True)\n",
    "        \n",
    "        \n",
    "    def encode(self, x):  # bs,1000,1  x1是特征，x2是风格\n",
    "        \n",
    "        x=x.transpose(-1,-2) # bs,1,1000\n",
    "        \n",
    "        xc=self.conv1(x)\n",
    "        xc=self.bn1(xc)\n",
    "        xc=self.ac1(xc)\n",
    "        xc=self.ac2(self.bn2(self.conv2(xc)))\n",
    "        xc=self.ac2(self.bn2(self.conv2(xc)))\n",
    "        xc=self.ac3(self.bn3(self.conv3(xc)))\n",
    "        xc=self.ac4(self.bn4(self.conv4(xc)))\n",
    "        xc=self.ac5(self.bn5(self.conv5(xc)))\n",
    "        xc=self.ac6(self.bn6(self.conv6(xc)))\n",
    "        xc=self.ac7(self.bn7(self.conv7(xc)))\n",
    "        xc=self.ac8(self.bn8(self.conv8(xc)))\n",
    "        xc=self.ac9(xc+self.bn9(self.conv9(xc)))\n",
    "        xc=self.ac10(xc+self.bn10(self.conv10(xc)))\n",
    "        content=self.ac11(xc+self.bn11(self.conv11(xc)))   # bs,64,1000\n",
    "\n",
    "        \n",
    "        \n",
    "        xs=self.conv_sty_1(x)\n",
    "        xs=self.ac_sty_1(xs)\n",
    "        xs=self.conv_sty_2(xs)\n",
    "        xs=self.ac_sty_2(xs)\n",
    "        xs=self.conv_sty_3(xs)\n",
    "        xs=self.ac_sty_3(xs)\n",
    "        xs=self.conv_sty_4(xs)\n",
    "        xs=self.ac_sty_4(xs)\n",
    "        xs=self.conv_sty_5(xs)\n",
    "        xs=self.ac_sty_5(xs)\n",
    "        xs=self.conv_sty_6(xs)\n",
    "        xs=self.ac_sty_6(xs)\n",
    "        xs=self.pool_sty(xs)\n",
    "        style=self.fc_sty(xs)\n",
    "        \n",
    "        return content, style\n",
    "    \n",
    "    def decode(self,content, style):  # x1逆向操作，x2应用到自适应归一化层\n",
    "        \n",
    "        adain_params = self.mlp_1(style.view(style.size(0), -1))\n",
    "        adain_params = self.mlp_2(adain_params)\n",
    "        adain_params = self.mlp_3(adain_params)\n",
    "        adain_params = self.mlp_4(adain_params)\n",
    "        adain_params = self.mlp_5(adain_params)\n",
    "        self.assign_adain_params(adain_params, self.dec)\n",
    "        x=self.dec(content)\n",
    "        x=x.transpose(-1,-2)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def assign_adain_params(self, adain_params, model):\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                mean = adain_params[:, :m.num_features]\n",
    "                std = adain_params[:, m.num_features:2*m.num_features]\n",
    "                m.bias = mean.contiguous().view(-1)\n",
    "                m.weight = std.contiguous().view(-1)\n",
    "                if adain_params.size(1) > 2*m.num_features:\n",
    "                    adain_params = adain_params[:, 2*m.num_features:]\n",
    "        \n",
    "        \n",
    "    def get_num_adain_params(self,model):\n",
    "        num_adain_params = 0\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                num_adain_params += 2*m.num_features\n",
    "        return num_adain_params\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # reconstruct an image\n",
    "        content, style_fake = self.encode(images)\n",
    "        images_recon = self.decode(content, style_fake)\n",
    "        return images_recon\n",
    "    \n",
    "    \n",
    "    \n",
    "class ECG_dis(nn.Module):  \n",
    "    def __init__(self,):  \n",
    "        super(ECG_dis, self).__init__()  \n",
    "        self.lead1_net=generate_model(base_model='resnet18',input_channels=1, num_classes=1,DG_method=None,domain_classes=6,distill=False)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.lead1_net(x)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "    def calc_dis_loss(self, input_fake, input_real):\n",
    "        # calculate the loss to train D\n",
    "        outs0 = self.forward(input_fake)\n",
    "        outs1 = self.forward(input_real)\n",
    "        loss = 0\n",
    "\n",
    "        for it, (out0, out1) in enumerate(zip(outs0, outs1)):\n",
    "            loss += torch.mean((out0 - 0)**2) + torch.mean((out1 - 1)**2)\n",
    "        return loss\n",
    "\n",
    "    def calc_gen_loss(self, input_fake):\n",
    "        # calculate the loss to train G\n",
    "        outs0 = self.forward(input_fake)\n",
    "        loss = 0\n",
    "        for it, (out0) in enumerate(outs0):\n",
    "            loss += torch.mean((out0 - 1)**2) # LSGAN\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class ECG_UNIT_Trainer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECG_UNIT_Trainer, self).__init__()\n",
    "        self.count=0\n",
    "        lr=0.0001\n",
    "        self.gen_a=ECG_gen().to(device)\n",
    "        self.gen_b=ECG_gen().to(device)\n",
    "        self.dis_a=ECG_dis().to(device)\n",
    "        self.dis_b=ECG_dis().to(device)\n",
    "        self.style_dim=8\n",
    "        \n",
    "        self.gen_loss=0\n",
    "        self.dis_loss=0\n",
    "        self.show_xba=None\n",
    "        self.show_xab=None\n",
    "        self.show_xa=None\n",
    "        self.show_xb=None\n",
    "        \n",
    "        \n",
    "        # fix the noise used in sampling\n",
    "        display_size = 16\n",
    "        self.s_a = torch.randn(display_size, self.style_dim).to(device)\n",
    "        self.s_b = torch.randn(display_size, self.style_dim).to(device)\n",
    "        \n",
    "        dis_params = list(self.dis_a.parameters()) + list(self.dis_b.parameters())\n",
    "        gen_params = list(self.gen_a.parameters()) + list(self.gen_b.parameters())\n",
    "        self.dis_opt = torch.optim.Adam([p for p in dis_params if p.requires_grad],\n",
    "                                        lr=lr, betas=(0.5, 0.999), weight_decay=0.0001)\n",
    "        self.gen_opt = torch.optim.Adam([p for p in gen_params if p.requires_grad],\n",
    "                                        lr=lr, betas=(0.5, 0.999), weight_decay=0.0001)\n",
    "        self.dis_scheduler = get_scheduler(self.dis_opt)\n",
    "        self.gen_scheduler = get_scheduler(self.gen_opt)\n",
    "        \n",
    "        self.apply(weights_init('kaiming'))\n",
    "        self.gen_b.apply(weights_init('kaiming'))\n",
    "        self.dis_a.apply(weights_init('gaussian'))\n",
    "        self.dis_b.apply(weights_init('gaussian'))\n",
    "\n",
    "    def recon_criterion(self, input, target):\n",
    "        return torch.mean(torch.abs(input - target))\n",
    "        \n",
    "    def forward(self, x_a, x_b):\n",
    "        self.eval()\n",
    "        s_a = Variable(self.s_a)\n",
    "        s_b = Variable(self.s_b)\n",
    "        c_a, s_a_fake = self.gen_a.encode(x_a)\n",
    "        c_b, s_b_fake = self.gen_b.encode(x_b)\n",
    "        x_ba = self.gen_a.decode(c_b, s_a)\n",
    "        x_ab = self.gen_b.decode(c_a, s_b)\n",
    "        self.train()\n",
    "        return x_ab, x_ba\n",
    "    \n",
    "    def gen_update(self, x_a, x_b):\n",
    "        self.gen_opt.zero_grad()\n",
    "        s_a = Variable(torch.randn(x_a.size(0), self.style_dim).to(device))\n",
    "        s_b = Variable(torch.randn(x_b.size(0), self.style_dim).to(device))\n",
    "        # encode\n",
    "        c_a, s_a_prime = self.gen_a.encode(x_a)\n",
    "        c_b, s_b_prime = self.gen_b.encode(x_b)\n",
    "        # decode (within domain)\n",
    "        x_a_recon = self.gen_a.decode(c_a, s_a_prime)\n",
    "        x_b_recon = self.gen_b.decode(c_b, s_b_prime)\n",
    "        # decode (cross domain)\n",
    "        x_ba = self.gen_a.decode(c_b, s_a)\n",
    "        x_ab = self.gen_b.decode(c_a, s_b)\n",
    "        # encode again\n",
    "        c_b_recon, s_a_recon = self.gen_a.encode(x_ba)\n",
    "        c_a_recon, s_b_recon = self.gen_b.encode(x_ab)\n",
    "\n",
    "        # reconstruction loss\n",
    "        self.loss_gen_recon_x_a = self.recon_criterion(x_a_recon, x_a)\n",
    "        self.loss_gen_recon_x_b = self.recon_criterion(x_b_recon, x_b)\n",
    "        self.loss_gen_recon_s_a = self.recon_criterion(s_a_recon, s_a)\n",
    "        self.loss_gen_recon_s_b = self.recon_criterion(s_b_recon, s_b)\n",
    "        self.loss_gen_recon_c_a = self.recon_criterion(c_a_recon, c_a)\n",
    "        self.loss_gen_recon_c_b = self.recon_criterion(c_b_recon, c_b)\n",
    "        # GAN loss\n",
    "        self.loss_gen_adv_a = self.dis_a.calc_gen_loss(x_ba)\n",
    "        self.loss_gen_adv_b = self.dis_b.calc_gen_loss(x_ab)\n",
    "        # total loss\n",
    "        self.loss_gen_total = 1 * self.loss_gen_adv_a + \\\n",
    "                              1 * self.loss_gen_adv_b + \\\n",
    "                              10 * self.loss_gen_recon_x_a + \\\n",
    "                              1 * self.loss_gen_recon_s_a + \\\n",
    "                              1 * self.loss_gen_recon_c_a + \\\n",
    "                              10 * self.loss_gen_recon_x_b + \\\n",
    "                              1 * self.loss_gen_recon_s_b + \\\n",
    "                              1 * self.loss_gen_recon_c_b\n",
    "\n",
    "        self.loss_gen_total.backward()\n",
    "        \n",
    "        if ((self.gen_loss)<(0.1*self.dis_loss))and(self.gen_loss!=0):\n",
    "            pass\n",
    "        else:\n",
    "            self.gen_opt.step()\n",
    "        self.gen_loss=self.loss_gen_total.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def dis_update(self, x_a, x_b):\n",
    "        self.dis_opt.zero_grad()\n",
    "        s_a = Variable(torch.randn(x_a.size(0), self.style_dim).to(device))\n",
    "        s_b = Variable(torch.randn(x_b.size(0), self.style_dim).to(device))\n",
    "        # encode\n",
    "        c_a, _ = self.gen_a.encode(x_a)\n",
    "        c_b, _ = self.gen_b.encode(x_b)\n",
    "        # decode (cross domain)\n",
    "        x_ba = self.gen_a.decode(c_b, s_a)\n",
    "        x_ab = self.gen_b.decode(c_a, s_b)\n",
    "        # D loss\n",
    "        self.loss_dis_a = self.dis_a.calc_dis_loss(x_ba.detach(), x_a)\n",
    "        self.loss_dis_b = self.dis_b.calc_dis_loss(x_ab.detach(), x_b)\n",
    "        self.loss_dis_total = 1 * self.loss_dis_a + 1 * self.loss_dis_b\n",
    "        self.loss_dis_total.backward()\n",
    "        \n",
    "        if ((self.gen_loss)>(10*self.dis_loss))and(self.dis_loss!=0):\n",
    "            pass\n",
    "        else:\n",
    "            self.dis_opt.step()\n",
    "#             if (self.count+1)%1000==0:\n",
    "        self.dis_loss=self.loss_dis_total.item()\n",
    "        self.show_xa=x_a.detach().cpu().numpy()\n",
    "        self.show_xb=x_b.detach().cpu().numpy()\n",
    "        self.show_xab=x_ab.detach().cpu().numpy()\n",
    "        self.show_xba=x_ba.detach().cpu().numpy()\n",
    "        \n",
    "    def show_loss(self,):\n",
    "        print('gen_loss=',self.gen_loss)\n",
    "        print('dis_loss=',self.dis_loss)\n",
    "        \n",
    "        with open('2024log0201_ECG_MBDG.txt', 'a') as file:  \n",
    "            file.write('______________'+str(self.count)+'_____________________________ \\n')\n",
    "            file.write('gen_loss \\n')\n",
    "            file.write(str(self.gen_loss)+'\\n')\n",
    "            file.write('dis_loss \\n')\n",
    "            file.write(str(self.dis_loss)+'\\n')\n",
    "            \n",
    "        plt.plot(self.show_xb[0,:])\n",
    "        plt.plot(self.show_xab[0,:])\n",
    "        plt.savefig('ECG_MBDG_figs/b_'+str(self.count)+'.png')\n",
    "        plt.show()\n",
    "        plt.plot(self.show_xa[0,:])\n",
    "        plt.plot(self.show_xba[0,:])\n",
    "        plt.savefig('ECG_MBDG_figs/a_'+str(self.count)+'.png')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        print('__________________________')\n",
    "        \n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        if self.dis_scheduler is not None:\n",
    "            self.dis_scheduler.step()\n",
    "            self.count+=1\n",
    "        if self.gen_scheduler is not None:\n",
    "            self.gen_scheduler.step()\n",
    "\n",
    "    def resume(self, checkpoint_dir):\n",
    "        # Load generators\n",
    "        last_model_name = get_model_list(checkpoint_dir, \"gen\")\n",
    "        state_dict = torch.load(last_model_name)\n",
    "        self.gen_a.load_state_dict(state_dict['a'])\n",
    "        self.gen_b.load_state_dict(state_dict['b'])\n",
    "        iterations = int(last_model_name[-11:-3])\n",
    "        # Load discriminators\n",
    "        last_model_name = get_model_list(checkpoint_dir, \"dis\")\n",
    "        state_dict = torch.load(last_model_name)\n",
    "        self.dis_a.load_state_dict(state_dict['a'])\n",
    "        self.dis_b.load_state_dict(state_dict['b'])\n",
    "        # Load optimizers\n",
    "        state_dict = torch.load(os.path.join(checkpoint_dir, 'optimizer.pt'))\n",
    "        self.dis_opt.load_state_dict(state_dict['dis'])\n",
    "        self.gen_opt.load_state_dict(state_dict['gen'])\n",
    "        # Reinitilize schedulers\n",
    "        self.dis_scheduler = get_scheduler(self.dis_opt,  iterations)\n",
    "        self.gen_scheduler = get_scheduler(self.gen_opt,  iterations)\n",
    "        print('Resume from iteration %d' % iterations)\n",
    "        return iterations\n",
    "\n",
    "    def save(self, snapshot_dir, iterations):\n",
    "        # Save generators, discriminators, and optimizers\n",
    "        gen_name = os.path.join(snapshot_dir, 'gen_%08d.pt' % (iterations + 1))\n",
    "        dis_name = os.path.join(snapshot_dir, 'dis_%08d.pt' % (iterations + 1))\n",
    "        opt_name = os.path.join(snapshot_dir, 'optimizer.pt')\n",
    "        torch.save({'a': self.gen_a.state_dict(), 'b': self.gen_b.state_dict()}, gen_name)\n",
    "        torch.save({'a': self.dis_a.state_dict(), 'b': self.dis_b.state_dict()}, dis_name)\n",
    "        torch.save({'gen': self.gen_opt.state_dict(), 'dis': self.dis_opt.state_dict()}, opt_name)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74686557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data L\n",
      "train data size : 16824\n",
      "train label class :  [13511.  1158.  2155. 16824.     0.     0.     0.     0.     0.]\n",
      "train data B\n",
      "train data size : 5501\n",
      "train label class :  [ 742.  859. 3900.    0. 5501.    0.    0.    0.    0.]\n",
      "train data C\n",
      "train data size : 24194\n",
      "train label class :  [ 6719.  2039. 15436.     0.     0. 24194.     0.     0.     0.]\n",
      "train data A\n",
      "train data size : 8274\n",
      "train label class :  [1393.  480. 6401.    0.    0.    0. 8274.    0.    0.]\n",
      "train data N\n",
      "train data size : 8516\n",
      "train label class :  [1462. 1769. 5285.    0.    0.    0.    0. 8516.    0.]\n",
      "train data H\n",
      "train data size : 20616\n",
      "train label class :  [11152.   557.  8907.     0.     0.     0.     0.     0. 20616.]\n",
      "total train data shape =  (83925, 1000, 1)\n",
      "total train label shape =  (83925, 9)\n",
      "total train data class =  [34979.  6862. 42084. 16824.  5501. 24194.  8274.  8516. 20616.]\n",
      "train data L\n",
      "train data size : 16824\n",
      "train label class :  [13511.  1158.  2155. 16824.     0.     0.     0.     0.     0.]\n",
      "train data B\n",
      "train data size : 5501\n",
      "train label class :  [ 742.  859. 3900.    0. 5501.    0.    0.    0.    0.]\n",
      "train data C\n",
      "train data size : 24194\n",
      "train label class :  [ 6719.  2039. 15436.     0.     0. 24194.     0.     0.     0.]\n",
      "train data A\n",
      "train data size : 8274\n",
      "train label class :  [1393.  480. 6401.    0.    0.    0. 8274.    0.    0.]\n",
      "train data N\n",
      "train data size : 8516\n",
      "train label class :  [1462. 1769. 5285.    0.    0.    0.    0. 8516.    0.]\n",
      "train data H\n",
      "train data size : 20616\n",
      "train label class :  [11152.   557.  8907.     0.     0.     0.     0.     0. 20616.]\n",
      "total train data shape =  (83925, 1000, 1)\n",
      "total train label shape =  (83925, 9)\n",
      "total train data class =  [34979.  6862. 42084. 16824.  5501. 24194.  8274.  8516. 20616.]\n"
     ]
    }
   ],
   "source": [
    "# 数据集加载\n",
    "\n",
    "train_data_list=['L','B','C','A','N','H']\n",
    "\n",
    "train_datasloader_instance=train_loader(dataname=train_data_list,train_val_test_split=[0.8,0.2,1],bs=1,num_workers=2,smote=False,maple=False,class_num=3)\n",
    "train_loader_a=train_datasloader_instance.loader()\n",
    "\n",
    "train_datasloader_instance=train_loader(dataname=train_data_list,train_val_test_split=[0.8,0.2,1],bs=1,num_workers=2,smote=False,maple=False,class_num=3)\n",
    "train_loader_b=train_datasloader_instance.loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1056a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     with Timer(\"Elapsed time in update: %f\"):\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#         # Main training code\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mdis_update(images_a, images_b)\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(iterations))\n",
      "Cell \u001b[0;32mIn[8], line 342\u001b[0m, in \u001b[0;36mECG_UNIT_Trainer.gen_update\u001b[0;34m(self, x_a, x_b)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# decode (cross domain)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m x_ba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_a\u001b[38;5;241m.\u001b[39mdecode(c_b, s_a)\n\u001b[0;32m--> 342\u001b[0m x_ab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# encode again\u001b[39;00m\n\u001b[1;32m    344\u001b[0m c_b_recon, s_a_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_a\u001b[38;5;241m.\u001b[39mencode(x_ba)\n",
      "Cell \u001b[0;32mIn[8], line 209\u001b[0m, in \u001b[0;36mECG_gen.decode\u001b[0;34m(self, content, style)\u001b[0m\n\u001b[1;32m    207\u001b[0m adain_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_5(adain_params)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_adain_params(adain_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec)\n\u001b[0;32m--> 209\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m x\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/software/miniconda3/envs/ecg_20230310/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 81\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mac3(x\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[1;32m     80\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mac4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x)))\n\u001b[0;32m---> 81\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mac5\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn5\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mac6(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn6(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv6(x)))\n\u001b[1;32m     83\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mac7(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn7(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv7(x)))\n",
      "File \u001b[0;32m~/software/miniconda3/envs/ecg_20230310/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/software/miniconda3/envs/ecg_20230310/lib/python3.10/site-packages/torch/nn/modules/activation.py:758\u001b[0m, in \u001b[0;36mLeakyReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_slope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/software/miniconda3/envs/ecg_20230310/lib/python3.10/site-packages/torch/nn/functional.py:1616\u001b[0m, in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(leaky_relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, negative_slope\u001b[38;5;241m=\u001b[39mnegative_slope, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 1616\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaky_relu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1618\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28minput\u001b[39m, negative_slope)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练流程，定期保存网络和优化器\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "max_iter = 4500000\n",
    "\n",
    "trainer = ECG_UNIT_Trainer()\n",
    "trainer.to(device)\n",
    "\n",
    "\n",
    "# 已经shuffle过了所以顺序没问题\n",
    "\n",
    "resume=False\n",
    "checkpoint_directory='save_model_states/'\n",
    "\n",
    "# Start training\n",
    "iterations = 0\n",
    "# = trainer.resume(checkpoint_directory) if resume else 0\n",
    "for epoch in range(50):\n",
    "    for it, (images_a_, images_b_) in enumerate(zip(train_loader_a, train_loader_b)):\n",
    "        trainer.update_learning_rate()\n",
    "        images_a, images_b = images_a_[0].float().to(device).detach(), images_b_[0].float().to(device).detach()\n",
    "\n",
    "    #     with Timer(\"Elapsed time in update: %f\"):\n",
    "    #         # Main training code\n",
    "        trainer.dis_update(images_a, images_b)\n",
    "        trainer.gen_update(images_a, images_b)\n",
    "\n",
    "\n",
    "        if (iterations + 1) % 1000 == 0:\n",
    "            print('iterations='+str(iterations))\n",
    "            trainer.show_loss()\n",
    "            print('__________________________________')\n",
    "\n",
    "        # Save network weights\n",
    "        if (iterations + 1) % 20000 == 0:\n",
    "            trainer.save(checkpoint_directory, iterations)\n",
    "\n",
    "        iterations += 1\n",
    "        if iterations >= max_iter:\n",
    "            sys.exit('Finish training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df60e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde5573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5f8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20854688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8ac61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae206c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28bdbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6825b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
