{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e482d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mvclab-gpuserver-57        \u001b[m  Sun Mar 24 19:59:53 2024  \u001b[1m\u001b[30m515.65.01\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 64'C\u001b[m, \u001b[1m\u001b[32m 50 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m13957\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30mxiaoyuzhou\u001b[m(\u001b[33m2347M\u001b[m) \u001b[1m\u001b[30myuzhenqin\u001b[m(\u001b[33m11297M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 68'C\u001b[m, \u001b[1m\u001b[32m 42 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m11442\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30myangli\u001b[m(\u001b[33m5157M\u001b[m) \u001b[1m\u001b[30mxiaoyuzhou\u001b[m(\u001b[33m2203M\u001b[m) \u001b[1m\u001b[30myangli\u001b[m(\u001b[33m3769M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 65'C\u001b[m, \u001b[32m 13 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m16073\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30myuzhenqin\u001b[m(\u001b[33m11993M\u001b[m) \u001b[1m\u001b[30myangli\u001b[m(\u001b[33m3769M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 67'C\u001b[m, \u001b[32m  1 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m15251\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30myuzhenqin\u001b[m(\u001b[33m11171M\u001b[m) \u001b[1m\u001b[30myangli\u001b[m(\u001b[33m3769M\u001b[m)\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 54'C\u001b[m, \u001b[32m 22 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14074\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30mrobotics\u001b[m(\u001b[33m13763M\u001b[m)\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 58'C\u001b[m, \u001b[32m 13 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14374\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30mrobotics\u001b[m(\u001b[33m14063M\u001b[m)\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 50'C\u001b[m, \u001b[32m 21 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14374\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30mrobotics\u001b[m(\u001b[33m14063M\u001b[m)\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 54'C\u001b[m, \u001b[32m 19 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14374\u001b[m / \u001b[33m24576\u001b[m MB | \u001b[1m\u001b[30mrobotics\u001b[m(\u001b[33m14063M\u001b[m)\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20890acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在之前的蒸馏基线上，加入ABCHLN单导联的GAN，生成4倍的数据，在ABCHLN验证，在TR进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c75baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.DG_models import generate_model\n",
    "from datasets import train_loader,val_loader,test_loader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] ='5'\n",
    "device='cuda:0'\n",
    "import wfdb\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import sklearn.metrics\n",
    "import csv\n",
    "import copy\n",
    "from utils import *\n",
    "import torch.nn.functional as F  \n",
    "from collections import deque  \n",
    "import random\n",
    "\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import wfdb\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics\n",
    "from scipy import signal\n",
    "import torch.nn.init as init\n",
    "from collections import deque  \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import defaultdict\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "from models.DG_models import generate_model\n",
    "from datasets import train_loader,val_loader,test_loader\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7840988",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr=2e-3\n",
    "gan_aug=3\n",
    "batch_size=128\n",
    "max_lr=base_lr*batch_size/256\n",
    "min_lr=max_lr/100\n",
    "distill=True\n",
    "DG_method_student='DG_GR'\n",
    "DG_method_teacher='teacher_self_distill_leadI_GRL'\n",
    "GRL_loss_beta=0.05\n",
    "DGGR_smooth_eps=0.3\n",
    "ts_label_beta=0.3\n",
    "ts_feature_beta=0.3\n",
    "train_val_test_split=[0.8,0.2,1] # 1 for inter\n",
    "base_optimizer=torch.optim.Adam\n",
    "loss_fn_clf=nn.CrossEntropyLoss()\n",
    "base_model_student='cnn_Ag'\n",
    "base_model_teacher='cnn_Ag_teacher_self_distill_leadI'\n",
    "class_num=3\n",
    "smote=False\n",
    "pair_num_per_anchor_batch=5\n",
    "CTL_margin=0.1\n",
    "warm_epoch=2\n",
    "total_epoch=30\n",
    "CRL_queue_len=5\n",
    "# 计算两个模型标签的余弦相似度损失\n",
    "\n",
    "class CosineSimilarityLoss(torch.nn.Module):  \n",
    "    def __init__(self):  \n",
    "        super(CosineSimilarityLoss, self).__init__()  \n",
    "  \n",
    "    def forward(self, input1, input2):  \n",
    "        cos_sim = torch.mean(F.cosine_similarity(input1, input2))\n",
    "        loss = 1 - cos_sim  \n",
    "        return loss\n",
    "\n",
    "loss_fn_tslb = CosineSimilarityLoss()\n",
    "\n",
    "# 基于欧氏距离的特征对比损失 \n",
    "\n",
    "# 没有温度系数没考虑，因为relu已经考虑了平滑的效果。\n",
    "\n",
    "# 如果想用温度系数建议去问大模型。\n",
    "  \n",
    "class ContrastiveLoss(torch.nn.Module):  \n",
    "    def __init__(self, margin=1.):  \n",
    "        super(ContrastiveLoss, self).__init__()  \n",
    "        self.margin = torch.tensor(margin, dtype=torch.float32)\n",
    "  \n",
    "    def forward(self, anchor, positive, negative):  \n",
    "#         print('anchor:',anchor)\n",
    "#         print('positive:',positive)\n",
    "#         print('negative:',negative)\n",
    "        # 计算锚点和正样本之间的余弦相似度  \n",
    "        similarity_anchor_positive = F.cosine_similarity(anchor, positive, dim=-1)  \n",
    "#         print('positive:',similarity_anchor_positive)\n",
    "        # 计算锚点和负样本之间的余弦相似度  \n",
    "        similarity_anchor_negative = F.cosine_similarity(anchor, negative, dim=-1)  \n",
    "#         print('negtive:',similarity_anchor_negative)\n",
    "  \n",
    "        # 计算损失  \n",
    "        loss = torch.mean(F.relu(similarity_anchor_positive-similarity_anchor_negative))\n",
    "#         print('cft_loss=',loss)\n",
    "        return loss\n",
    "\n",
    "feature_contrastive_loss_fn = ContrastiveLoss(margin=CTL_margin)\n",
    "\n",
    "# 维护教师的特征内存队列和标签队列\n",
    "\n",
    "class TeacherQueue:  \n",
    "    def __init__(self, max_size):  \n",
    "        self.max_size = max_size  \n",
    "        self.feat_queue = deque(maxlen=max_size)  \n",
    "        self.label_queue = deque(maxlen=max_size)  \n",
    "  \n",
    "    def enqueue(self, features,labels):  \n",
    "        \"\"\"  \n",
    "        将特征向量加入队列  \n",
    "        :param features: 特征向量，形状为[batch_size, feature_dim]  \n",
    "        \"\"\"  \n",
    "        if len(self.feat_queue) == self.max_size:  \n",
    "            self.dequeue()  # 如果队列已满，先出队  \n",
    "        self.feat_queue.append(features)  \n",
    "        self.label_queue.append(labels)  \n",
    "  \n",
    "    def dequeue(self):  \n",
    "        \"\"\"  \n",
    "        从队列中移除最早加入的特征向量，无需返回值。  \n",
    "        \"\"\"  \n",
    "        self.feat_queue.popleft()  \n",
    "        self.label_queue.popleft()  \n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def get_queue_len(self):\n",
    "        return len(self.feat_queue)\n",
    "  \n",
    "    def get_all_queue(self):  \n",
    "        \"\"\"  \n",
    "        返回队列中所有的特征向量和标签，形状为[total_size, feature_dim],[total_size, label_dim]  \n",
    "        \"\"\"  \n",
    "        return torch.cat(list(self.feat_queue)),torch.cat(list(self.label_queue))\n",
    "\n",
    "def get_class_f1(tar,pred):\n",
    "    # 多分类多标签或单标签，输入为概率或one hot\n",
    "    pred_idx = pred.argmax(axis=1)\n",
    "    pred_=(pred_idx[:,None] == np.arange(pred.shape[1])).astype(int)\n",
    "    F1_normal_score,F1_AF_score,F1_other_score=list(sklearn.metrics.f1_score(tar, pred_, average=None))\n",
    "    return F1_normal_score,F1_AF_score,F1_other_score\n",
    "\n",
    "def val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          test_datasloader, device):\n",
    "    \n",
    "    student_model.eval() \n",
    "    running_loss = None \n",
    "    test_pred=0\n",
    "    running_loss = []\n",
    "    test_tar=0\n",
    "    for step, (in_data_, targets_) in (enumerate(test_datasloader)):\n",
    "        tmp_data=np.zeros((in_data_.shape[0],1000,2))\n",
    "        tmp_data[:,:,0]=in_data_[:,:,0]\n",
    "        tmp_data[:,:-1,1]=in_data_[:,1:,0]-in_data_[:,0:-1,0]\n",
    "        preds_,_,_=student_model(torch.tensor(tmp_data).float().to(device))\n",
    "        preds_=preds_.float()\n",
    "        targets_for_loss=torch.max(targets_[:,:class_num],1).indices.long().to(device)\n",
    "        loss=loss_fn_clf(preds_,targets_for_loss)    \n",
    "        running_loss.append(loss.item())\n",
    "        mean_loss=np.mean(running_loss)\n",
    "\n",
    "        # 使用AUC作为评价指标\n",
    "        if type(test_pred)==int:\n",
    "            test_pred=preds_.clone().detach().cpu().numpy()\n",
    "        else:\n",
    "            test_pred=np.concatenate((test_pred,\n",
    "                                preds_.clone().detach().cpu().numpy(),\n",
    "                                ),axis=0)\n",
    "        if type(test_tar)==int:\n",
    "            test_tar=targets_.clone().detach().cpu().numpy()\n",
    "        else:\n",
    "            test_tar=np.concatenate((test_tar,\n",
    "                                targets_.clone().detach().cpu().numpy(),\n",
    "                                ),axis=0)\n",
    "\n",
    "    print('epoch=',epoch,)\n",
    "    print(\"val_loss=\",mean_loss)\n",
    "    print(\"________________________________\")\n",
    "    mean_macro_auc=sklearn.metrics.roc_auc_score(test_tar, test_pred, average='macro')\n",
    "#     print('test mean_macro_auc= ',mean_macro_auc)\n",
    "    \n",
    "#     print(test_tar.shape)\n",
    "#     print(test_pred.shape)\n",
    "    \n",
    "    F1_score=get_macro_f1(test_tar,test_pred)\n",
    "#     print('test F1_score= ',F1_score)\n",
    "    \n",
    "    precision=get_macro_precision(test_tar,test_pred)\n",
    "#     print('test precision= ',precision)\n",
    "    \n",
    "    recall=get_macro_recall(test_tar,test_pred)\n",
    "#     print('test recall= ',recall)\n",
    "\n",
    "    acc=get_acc(test_tar,test_pred)\n",
    "#     print('test acc= ',acc)\n",
    "    \n",
    "    F1_normal_score,F1_AF_score,F1_other_score=get_class_f1(test_tar, test_pred)\n",
    "    return str(int(round(1000*mean_macro_auc))),str(int(round(1000*precision))),str(int(round(1000*recall))),str(int(round(1000*F1_score))),str(int(round(1000*F1_normal_score))),str(int(round(1000*F1_AF_score))),str(int(round(1000*F1_other_score))),str(int(round(1000*acc)))\n",
    "\n",
    "def test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          test_datasloader, device):\n",
    "    \n",
    "    student_model.eval() \n",
    "    running_loss = None \n",
    "    test_pred=0\n",
    "    running_loss = []\n",
    "    test_tar=0\n",
    "    for step, (in_data_, targets_) in (enumerate(test_datasloader)):\n",
    "        tmp_data=np.zeros((in_data_.shape[0],1000,2))\n",
    "        tmp_data[:,:,0]=in_data_[:,:,0]\n",
    "        tmp_data[:,:-1,1]=in_data_[:,1:,0]-in_data_[:,0:-1,0]\n",
    "        preds_,_,_=student_model(torch.tensor(tmp_data).float().to(device))\n",
    "        preds_=preds_.float()\n",
    "        targets_for_loss=torch.max(targets_[:,:class_num],1).indices.long().to(device)\n",
    "        loss=loss_fn_clf(preds_,targets_for_loss)    \n",
    "        running_loss.append(loss.item())\n",
    "        mean_loss=np.mean(running_loss)\n",
    "\n",
    "        # 使用AUC作为评价指标\n",
    "        if type(test_pred)==int:\n",
    "            test_pred=preds_.clone().detach().cpu().numpy()\n",
    "        else:\n",
    "            test_pred=np.concatenate((test_pred,\n",
    "                                preds_.clone().detach().cpu().numpy(),\n",
    "                                ),axis=0)\n",
    "        if type(test_tar)==int:\n",
    "            test_tar=targets_.clone().detach().cpu().numpy()\n",
    "        else:\n",
    "            test_tar=np.concatenate((test_tar,\n",
    "                                targets_.clone().detach().cpu().numpy(),\n",
    "                                ),axis=0)\n",
    "\n",
    "    print('epoch=',epoch,)\n",
    "    print(\"test_loss=\",mean_loss)\n",
    "    print(\"________________________________\")\n",
    "    mean_macro_auc=sklearn.metrics.roc_auc_score(test_tar, test_pred, average='macro')\n",
    "#     print('test mean_macro_auc= ',mean_macro_auc)\n",
    "    \n",
    "#     print(test_tar.shape)\n",
    "#     print(test_pred.shape)\n",
    "    \n",
    "    F1_score=get_macro_f1(test_tar,test_pred)\n",
    "#     print('test F1_score= ',F1_score)\n",
    "    \n",
    "    precision=get_macro_precision(test_tar,test_pred)\n",
    "#     print('test precision= ',precision)\n",
    "    \n",
    "    recall=get_macro_recall(test_tar,test_pred)\n",
    "#     print('test recall= ',recall)\n",
    "\n",
    "    acc=get_acc(test_tar,test_pred)\n",
    "#     print('test acc= ',acc)\n",
    "    \n",
    "    F1_normal_score,F1_AF_score,F1_other_score=get_class_f1(test_tar, test_pred)\n",
    "    return str(int(round(1000*mean_macro_auc))),str(int(round(1000*precision))),str(int(round(1000*recall))),str(int(round(1000*F1_score))),str(int(round(1000*F1_normal_score))),str(int(round(1000*F1_AF_score))),str(int(round(1000*F1_other_score))),str(int(round(1000*acc)))\n",
    "\n",
    "tem_relu=nn.ReLU(inplace=False)\n",
    "\n",
    "# 定义训练流程\n",
    "def train_student_one_epoch(epoch,warm_epoch,total_epoch,student_model,teacher_model, \\\n",
    "                            max_lr,min_lr,loss_fn_clf,loss_fn_tslb,optimizer, \\\n",
    "                            feature_contrastive_loss_fn, \\\n",
    "                            Teacher_queue,base_optimizer, \\\n",
    "                            train_datasloader,device,GRL_loss_beta,GAN_aug=None,GAN_model=None):\n",
    "    student_model.train()\n",
    "    running_loss = None \n",
    "    # 维护一个对比损失的内存队列\n",
    "    for step, (in_data_, targets_) in (enumerate(tqdm(train_datasloader))):\n",
    "        # 教师模型的预测\n",
    "        \n",
    "        assert (gan_aug==3) # 进行一次教师模型的前向传播和队列存储，一次原有单导联蒸馏，3次单导联生成蒸馏\n",
    "        \n",
    "        teacher_diag_preds_,_,_,teacher_feature,_,_,_,_=teacher_model(in_data_.float().to(device))\n",
    "        teacher_feature=torch.flatten(teacher_feature,1).detach()\n",
    "#         teacher_feature=tem_relu(teacher_feature).detach()\n",
    "#         print(teacher_feature.shape)\n",
    "        # 学生模型的预测\n",
    "        current_iter=step+epoch*len(train_datasloader)\n",
    "        warm_iter=warm_epoch*len(train_datasloader)\n",
    "        total_iter=total_epoch*len(train_datasloader)\n",
    "        adjust_learning_rate(optimizer,current_iter,warm_iter,total_iter,max_lr,min_lr)\n",
    "        \n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             in_data_1 = in_data_[:,:,:1].float().to(device).clone()\n",
    "#             in_data_2 = GAN_model.generate_images(torch.cat((in_data_1,in_data_1,in_data_1),dim=0))  # 32,\n",
    "\n",
    "#             in_data_GAN4=torch.cat((in_data_1,in_data_2),dim=0)\n",
    "                                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        tmp_data=np.zeros((in_data_.shape[0],1000,2))\n",
    "        tmp_data[:,:,0]=in_data_[:,:,0]\n",
    "        tmp_data[:,:-1,1]=in_data_[:,1:,0]-in_data_[:,0:-1,0]\n",
    "        student_diag_preds_,student_domain_preds_,student_feature=student_model(torch.tensor(tmp_data).float().to(device))\n",
    "        student_diag_preds_=student_diag_preds_.float()\n",
    "        student_domain_preds_=student_domain_preds_.float()\n",
    "        teacher_feature=teacher_feature.float()\n",
    "        current_label=targets_[:,:class_num].clone()\n",
    "        Teacher_queue.enqueue(teacher_feature,current_label)\n",
    "        student_feature=torch.flatten(student_feature,1).float()\n",
    "\n",
    "        # 损失函数的计算和反向传播\n",
    "        # 分类损失部分\n",
    "        targets_for_diag_loss=torch.max(targets_[:,:class_num],1).indices.long().to(device)  # 这里的2是指两类。\n",
    "        loss_diag=loss_fn_clf(student_diag_preds_,targets_for_diag_loss)\n",
    "        # domain损失部分\n",
    "        targets_for_domain_loss=torch.max(targets_[:,class_num+1:],1).indices.long().to(device)\n",
    "                            # 这里的3是指后面的领域情况。\n",
    "        loss_domain=loss_fn_clf(student_domain_preds_,targets_for_domain_loss)\n",
    "#         # 分类结果差异损失部分\n",
    "        loss_tslb = loss_fn_tslb(teacher_diag_preds_.detach(), student_diag_preds_)  \n",
    "#         # 特征对比损失部分\n",
    "#         # 获取教师模型保存的特征向量\n",
    "        if (Teacher_queue.get_queue_len()==0):\n",
    "            pass\n",
    "        else:\n",
    "            all_teacher_feature,all_teacher_labels=Teacher_queue.get_all_queue()\n",
    "            if  torch.allclose(all_teacher_feature, all_teacher_feature[0]) : # 队列中只有一类标签  \n",
    "                pass\n",
    "            else:\n",
    "#                 print('loss_tsft:',1)\n",
    "                feature_contrastive_loss_list=[]\n",
    "                p_teacher_set=torch.zeros(pair_num_per_anchor_batch,current_label.shape[0],student_feature.shape[-1]).to(device)\n",
    "                n_teacher_set=torch.zeros(pair_num_per_anchor_batch,current_label.shape[0],student_feature.shape[-1]).to(device)\n",
    "\n",
    "#                 student_feature\n",
    "#                 current_label\n",
    "                for tmp_stu_idx in range(current_label.shape[0]):\n",
    "                    # 获取正样本索引元组\n",
    "#                     print('current_label[tmp_stu_idx]',current_label[tmp_stu_idx])\n",
    "#                     print('all_teacher_labels',all_teacher_labels)\n",
    "                    positive_idxs= [u for u, label in enumerate(all_teacher_labels[:, 0]+2*all_teacher_labels[:, 1]) \\\n",
    "                                     if label == (current_label[tmp_stu_idx,0].item()+2*current_label[tmp_stu_idx,1].item())]\n",
    "                    \n",
    "#                     print(positive_idxs)\n",
    "#                     for u,label in enumerate(all_teacher_labels[:, 0]+2*all_teacher_labels[:, 1]):\n",
    "#                         print( u,label )\n",
    "#                         print(current_label[tmp_stu_idx,0].item()+2*current_label[tmp_stu_idx,1].item())\n",
    "#                     print(current_label[tmp_stu_idx,0].item()+2*current_label[tmp_stu_idx,1].item())\n",
    "#                         print('_______________________________________________')\n",
    "                    \n",
    "#                     print('positive_idxs',positive_idxs)\n",
    "                    # 获取负样本索引元组\n",
    "                    negative_idxs=[x for x in range(all_teacher_labels.shape[0]) if x not in positive_idxs]  \n",
    "#                     print('negative_idxs',negative_idxs)\n",
    "                    for iii in range(pair_num_per_anchor_batch):\n",
    "                        p_teacher_set[iii,tmp_stu_idx,:]=all_teacher_feature[random.choice(positive_idxs)]\n",
    "                        n_teacher_set[iii,tmp_stu_idx,:]=all_teacher_feature[random.choice(negative_idxs)]\n",
    "                for iii in range(pair_num_per_anchor_batch):\n",
    "                    feature_contrastive_loss_list.append(feature_contrastive_loss_fn(student_feature, p_teacher_set[iii], n_teacher_set[iii]))\n",
    "                loss_tsft=torch.mean(torch.tensor(feature_contrastive_loss_list))        \n",
    "                        \n",
    "\n",
    "\n",
    "        # 求和\n",
    "#         print(loss_diag)\n",
    "#         print(loss_domain)\n",
    "#         print(loss_tslb)\n",
    "#         print(loss_tsft)\n",
    "#         print('___________________')\n",
    "#         loss=loss_diag+GRL_loss_beta*loss_domain+ts_label_beta*loss_tslb\n",
    "        if (loss_tsft):\n",
    "            loss=loss_diag+GRL_loss_beta*loss_domain+ts_label_beta*loss_tslb+ts_feature_beta*loss_tsft\n",
    "        else:\n",
    "            loss=loss_diag+GRL_loss_beta*loss_domain+ts_label_beta*loss_tslb\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if running_loss is None:\n",
    "            running_loss = loss.item() # 只输出重构损失\n",
    "        else:\n",
    "            running_loss = running_loss * .9 + loss.item() * .1   \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        for gan_i in range(gan_aug):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                in_data_1 = in_data_[:,:,:1].float().to(device).clone()\n",
    "                in_data_gan = GAN_model.generate_images(in_data_1).cpu().numpy()  # 32,\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            tmp_data=np.zeros((in_data_gan.shape[0],1000,2))\n",
    "            tmp_data[:,:,0]=in_data_gan[:,:,0]\n",
    "            tmp_data[:,:-1,1]=in_data_gan[:,1:,0]-in_data_gan[:,0:-1,0]\n",
    "            student_diag_preds_,student_domain_preds_,student_feature=student_model(torch.tensor(tmp_data).float().to(device))\n",
    "            student_diag_preds_=student_diag_preds_.float()\n",
    "            student_domain_preds_=student_domain_preds_.float()\n",
    "            student_feature=torch.flatten(student_feature,1).float()\n",
    "\n",
    "            # 损失函数的计算和反向传播\n",
    "            # 分类损失部分\n",
    "            targets_for_diag_loss=torch.max(targets_[:,:class_num],1).indices.long().to(device)  # 这里的2是指两类。\n",
    "            loss_diag=loss_fn_clf(student_diag_preds_,targets_for_diag_loss)\n",
    "            # domain损失部分\n",
    "            targets_for_domain_loss=torch.max(targets_[:,class_num+1:],1).indices.long().to(device)\n",
    "                                # 这里的3是指后面的领域情况。\n",
    "            loss_domain=loss_fn_clf(student_domain_preds_,targets_for_domain_loss)\n",
    "    #         # 分类结果差异损失部分\n",
    "            loss_tslb = loss_fn_tslb(teacher_diag_preds_.detach(), student_diag_preds_)  \n",
    "    #         # 特征对比损失部分\n",
    "    #         # 获取教师模型保存的特征向量\n",
    "            if (Teacher_queue.get_queue_len()==0):\n",
    "                pass\n",
    "            else:\n",
    "                all_teacher_feature,all_teacher_labels=Teacher_queue.get_all_queue()\n",
    "                if  torch.allclose(all_teacher_feature, all_teacher_feature[0]) : # 队列中只有一类标签  \n",
    "                    pass\n",
    "                else:\n",
    "    #                 print('loss_tsft:',1)\n",
    "                    feature_contrastive_loss_list=[]\n",
    "                    p_teacher_set=torch.zeros(pair_num_per_anchor_batch,current_label.shape[0],student_feature.shape[-1]).to(device)\n",
    "                    n_teacher_set=torch.zeros(pair_num_per_anchor_batch,current_label.shape[0],student_feature.shape[-1]).to(device)\n",
    "\n",
    "    #                 student_feature\n",
    "    #                 current_label\n",
    "                    for tmp_stu_idx in range(current_label.shape[0]):\n",
    "                        # 获取正样本索引元组\n",
    "    #                     print('current_label[tmp_stu_idx]',current_label[tmp_stu_idx])\n",
    "    #                     print('all_teacher_labels',all_teacher_labels)\n",
    "                        positive_idxs= [u for u, label in enumerate(all_teacher_labels[:, 0]+2*all_teacher_labels[:, 1]) \\\n",
    "                                         if label == (current_label[tmp_stu_idx,0].item()+2*current_label[tmp_stu_idx,1].item())]\n",
    "\n",
    "    #                     print(positive_idxs)\n",
    "    #                     for u,label in enumerate(all_teacher_labels[:, 0]+2*all_teacher_labels[:, 1]):\n",
    "    #                         print( u,label )\n",
    "    #                         print(current_label[tmp_stu_idx,0].item()+2*current_label[tmp_stu_idx,1].item())\n",
    "    #                     print(current_label[tmp_stu_idx,0].item()+2*current_label[tmp_stu_idx,1].item())\n",
    "    #                         print('_______________________________________________')\n",
    "\n",
    "    #                     print('positive_idxs',positive_idxs)\n",
    "                        # 获取负样本索引元组\n",
    "                        negative_idxs=[x for x in range(all_teacher_labels.shape[0]) if x not in positive_idxs]  \n",
    "    #                     print('negative_idxs',negative_idxs)\n",
    "                        for iii in range(pair_num_per_anchor_batch):\n",
    "                            p_teacher_set[iii,tmp_stu_idx,:]=all_teacher_feature[random.choice(positive_idxs)]\n",
    "                            n_teacher_set[iii,tmp_stu_idx,:]=all_teacher_feature[random.choice(negative_idxs)]\n",
    "                    for iii in range(pair_num_per_anchor_batch):\n",
    "                        feature_contrastive_loss_list.append(feature_contrastive_loss_fn(student_feature, p_teacher_set[iii], n_teacher_set[iii]))\n",
    "                    loss_tsft=torch.mean(torch.tensor(feature_contrastive_loss_list))        \n",
    "\n",
    "\n",
    "\n",
    "            # 求和\n",
    "    #         print(loss_diag)\n",
    "    #         print(loss_domain)\n",
    "    #         print(loss_tslb)\n",
    "    #         print(loss_tsft)\n",
    "    #         print('___________________')\n",
    "    #         loss=loss_diag+GRL_loss_beta*loss_domain+ts_label_beta*loss_tslb\n",
    "            if (loss_tsft):\n",
    "                loss=loss_diag+GRL_loss_beta*loss_domain+ts_label_beta*loss_tslb+ts_feature_beta*loss_tsft\n",
    "            else:\n",
    "                loss=loss_diag+GRL_loss_beta*loss_domain+ts_label_beta*loss_tslb\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item() # 只输出重构损失\n",
    "            else:\n",
    "                running_loss = running_loss * .9 + loss.item() * .1   \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    print('epoch=',epoch,)\n",
    "    print(\"train_running_loss=\",running_loss)\n",
    "\n",
    "            # loss的打印\n",
    "            \n",
    "    # 特征对比损失部分    \n",
    "    # 分类结果差异损失部分\n",
    "    # domain损失部分\n",
    "    # 分类损失部分\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743e2c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model=cnn_Ag\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MBDG(\n",
       "  (network): CNN_Ag(\n",
       "    (conv1): Conv1d_(1, 32, kernel_size=(16,), stride=(1,), bias=False)\n",
       "    (res_layers): Sequential(\n",
       "      (0): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(32, 32, kernel_size=(16,), stride=(1,))\n",
       "        (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(32, 32, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (1): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(32, 32, kernel_size=(16,), stride=(2,))\n",
       "        (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(32, 32, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (2): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(32, 32, kernel_size=(16,), stride=(1,))\n",
       "        (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(32, 32, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (3): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(32, 32, kernel_size=(16,), stride=(2,))\n",
       "        (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(32, 32, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (4): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(32, 64, kernel_size=(16,), stride=(1,))\n",
       "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(64, 64, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (5): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(64, 64, kernel_size=(16,), stride=(2,))\n",
       "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(64, 64, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (6): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(64, 64, kernel_size=(16,), stride=(1,))\n",
       "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(64, 64, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (7): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(64, 64, kernel_size=(16,), stride=(2,))\n",
       "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(64, 64, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (8): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(64, 128, kernel_size=(16,), stride=(1,))\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(128, 128, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (9): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(128, 128, kernel_size=(16,), stride=(2,))\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(128, 128, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (10): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(128, 128, kernel_size=(16,), stride=(1,))\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(128, 128, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (11): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(128, 128, kernel_size=(16,), stride=(2,))\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(128, 128, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (12): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(128, 256, kernel_size=(16,), stride=(1,))\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(256, 256, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (13): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(256, 256, kernel_size=(16,), stride=(2,))\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(256, 256, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (14): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(256, 256, kernel_size=(16,), stride=(1,))\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(256, 256, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "      (15): Block_Ag(\n",
       "        (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Conv1d_(256, 256, kernel_size=(16,), stride=(2,))\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (conv2): Conv1d_(256, 256, kernel_size=(16,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (fc): Linear(in_features=256, out_features=3, bias=True)\n",
       "    (fc_1_domain): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (fc_2_domain): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (relu_domain): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (dp_domain): Dropout(p=0.5, inplace=False)\n",
       "    (fc_1_domain_n): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (fc_2_domain_n): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (relu_domain_n): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (dp_domain_n): Dropout(p=0.5, inplace=False)\n",
       "    (fc_1_domain_a): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (fc_2_domain_a): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (relu_domain_a): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (dp_domain_a): Dropout(p=0.5, inplace=False)\n",
       "    (fc_1_domain_o): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (fc_2_domain_o): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (relu_domain_o): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (dp_domain_o): Dropout(p=0.5, inplace=False)\n",
       "    (grl): GRL()\n",
       "    (grl_class): GRL()\n",
       "  )\n",
       "  (G): MUNITModelOfNatVar(\n",
       "    (_gen_A): ECG_gen(\n",
       "      (conv1): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv2): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv3): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn3): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv4): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn4): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv5): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn5): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv6): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn6): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv7): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn7): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn8): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac8): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn9): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac9): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv10): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn10): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac10): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv11): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn11): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_1): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_2): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_4): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_5): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (pool_sty): AdaptiveAvgPool1d(output_size=1)\n",
       "      (fc_sty): Conv1d(256, 8, kernel_size=(1,), stride=(1,))\n",
       "      (dec): Decoder(\n",
       "        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn1): AdaptiveInstanceNorm2d(64)\n",
       "        (ac1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn2): AdaptiveInstanceNorm2d(64)\n",
       "        (ac2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn3): AdaptiveInstanceNorm2d(64)\n",
       "        (ac3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn4): AdaptiveInstanceNorm2d(32)\n",
       "        (ac4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn5): AdaptiveInstanceNorm2d(32)\n",
       "        (ac5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv6): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn6): AdaptiveInstanceNorm2d(16)\n",
       "        (ac6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv7): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn7): AdaptiveInstanceNorm2d(16)\n",
       "        (ac7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv8): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn8): AdaptiveInstanceNorm2d(8)\n",
       "        (ac8): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv9): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (ac9): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv10): Conv1d(8, 1, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (mlp_1): Linear(in_features=8, out_features=256, bias=True)\n",
       "      (mlp_2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (mlp_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (mlp_4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (mlp_5): Linear(in_features=256, out_features=592, bias=True)\n",
       "    )\n",
       "    (_gen_B): ECG_gen(\n",
       "      (conv1): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv2): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv3): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn3): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv4): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn4): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv5): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn5): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv6): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn6): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv7): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn7): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv8): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn8): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac8): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn9): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac9): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv10): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn10): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac10): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv11): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn11): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (ac11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_1): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_2): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_4): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_5): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv_sty_6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (ac_sty_6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (pool_sty): AdaptiveAvgPool1d(output_size=1)\n",
       "      (fc_sty): Conv1d(256, 8, kernel_size=(1,), stride=(1,))\n",
       "      (dec): Decoder(\n",
       "        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn1): AdaptiveInstanceNorm2d(64)\n",
       "        (ac1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn2): AdaptiveInstanceNorm2d(64)\n",
       "        (ac2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn3): AdaptiveInstanceNorm2d(64)\n",
       "        (ac3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn4): AdaptiveInstanceNorm2d(32)\n",
       "        (ac4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn5): AdaptiveInstanceNorm2d(32)\n",
       "        (ac5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv6): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn6): AdaptiveInstanceNorm2d(16)\n",
       "        (ac6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv7): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn7): AdaptiveInstanceNorm2d(16)\n",
       "        (ac7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv8): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn8): AdaptiveInstanceNorm2d(8)\n",
       "        (ac8): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv9): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (ac9): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv10): Conv1d(8, 1, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (mlp_1): Linear(in_features=8, out_features=256, bias=True)\n",
       "      (mlp_2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (mlp_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (mlp_4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (mlp_5): Linear(in_features=256, out_features=592, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成模型定义，包括两个encoder和一个decoder\n",
    "\n",
    "class AdaptiveInstanceNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(AdaptiveInstanceNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # weight and bias are dynamically assigned\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        # just dummy buffers, not used\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.weight is not None and self.bias is not None, \"Please assign weight and bias before calling AdaIN!\"\n",
    "        b, c = x.size(0), x.size(1)\n",
    "        running_mean = self.running_mean.repeat(b)\n",
    "        running_var = self.running_var.repeat(b)\n",
    "\n",
    "        # Apply instance norm\n",
    "        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n",
    "        out = F.batch_norm(\n",
    "            x_reshaped, running_mean, running_var, self.weight, self.bias,\n",
    "            True, self.momentum, self.eps)\n",
    "\n",
    "        return out.view(b, c, *x.size()[2:])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + str(self.num_features) + ')'\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # dec\n",
    "        self.conv1=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn1=AdaptiveInstanceNorm2d(64)\n",
    "        self.ac1=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv2=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2=AdaptiveInstanceNorm2d(64)\n",
    "        self.ac2=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv3=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn3=AdaptiveInstanceNorm2d(64)\n",
    "        self.ac3=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv4=nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn4=AdaptiveInstanceNorm2d(32)\n",
    "        self.ac4=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv5=nn.Conv1d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn5=AdaptiveInstanceNorm2d(32)\n",
    "        self.ac5=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv6=nn.Conv1d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn6=AdaptiveInstanceNorm2d(16)\n",
    "        self.ac6=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv7=nn.Conv1d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn7=AdaptiveInstanceNorm2d(16)\n",
    "        self.ac7=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv8=nn.Conv1d(16, 8, kernel_size=3, padding=1)\n",
    "        self.bn8=AdaptiveInstanceNorm2d(8)\n",
    "        self.ac8=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv9=nn.Conv1d(8, 8, kernel_size=3, padding=1)\n",
    "        self.ac9=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv10=nn.Conv1d(8, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x=self.ac1(x+self.bn1(self.conv1(x)))\n",
    "        x=self.ac2(x+self.bn2(self.conv2(x)))\n",
    "        x=self.ac3(x+self.bn3(self.conv3(x)))\n",
    "        x=self.ac4(self.bn4(self.conv4(x)))\n",
    "        x=self.ac5(self.bn5(self.conv5(x)))\n",
    "        x=self.ac6(self.bn6(self.conv6(x)))\n",
    "        x=self.ac7(self.bn7(self.conv7(x)))\n",
    "        x=self.ac8(self.bn8(self.conv8(x)))\n",
    "        x=self.conv10(self.ac9(self.conv9(x)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ECG_gen(nn.Module):  \n",
    "    def __init__(self,):  \n",
    "        super(ECG_gen, self).__init__()  \n",
    "        \n",
    "        # cont_enc\n",
    "        \n",
    "        self.conv1=nn.Conv1d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1=nn.InstanceNorm1d(8)\n",
    "        self.ac1=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv2=nn.Conv1d(8, 8, kernel_size=3, padding=1)\n",
    "        self.bn2=nn.InstanceNorm1d(8)\n",
    "        self.ac2=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv3=nn.Conv1d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn3=nn.InstanceNorm1d(16)\n",
    "        self.ac3=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv4=nn.Conv1d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn4=nn.InstanceNorm1d(16)\n",
    "        self.ac4=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv5=nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn5=nn.InstanceNorm1d(32)\n",
    "        self.ac5=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv6=nn.Conv1d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn6=nn.InstanceNorm1d(32)\n",
    "        self.ac6=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv7=nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn7=nn.InstanceNorm1d(64)\n",
    "        self.ac7=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv8=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn8=nn.InstanceNorm1d(64)\n",
    "        self.ac8=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv9=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn9=nn.InstanceNorm1d(64)\n",
    "        self.ac9=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv10=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn10=nn.InstanceNorm1d(64)\n",
    "        self.ac10=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv11=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn11=nn.InstanceNorm1d(64)\n",
    "        self.ac11=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        # style_enc\n",
    "        \n",
    "        self.conv_sty_1=nn.Conv1d(1, 8, kernel_size=3, padding=1)\n",
    "        self.ac_sty_1=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_2=nn.Conv1d(8, 16, kernel_size=3, padding=1)\n",
    "        self.ac_sty_2=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_3=nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.ac_sty_3=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_4=nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.ac_sty_4=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_5=nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.ac_sty_5=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_6=nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.ac_sty_6=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.pool_sty=nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc_sty=nn.Conv1d(256, 8, kernel_size=1, padding=0)\n",
    "        # bs,8,1\n",
    "        \n",
    "        self.dec=Decoder()\n",
    "        \n",
    "        self.mlp_1=nn.Linear(8, 256, bias=True)\n",
    "        self.mlp_2=nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.mlp_3=nn.Linear(256, 256, bias=True)\n",
    "        self.mlp_4=nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.mlp_5=nn.Linear(256, self.get_num_adain_params(self.dec), bias=True)\n",
    "        \n",
    "        \n",
    "    def encode(self, x):  # bs,1000,1  x1是特征，x2是风格\n",
    "        \n",
    "        x=x.transpose(-1,-2) # bs,1,1000\n",
    "        \n",
    "        xc=self.conv1(x)\n",
    "        xc=self.bn1(xc)\n",
    "        xc=self.ac1(xc)\n",
    "        xc=self.ac2(self.bn2(self.conv2(xc)))\n",
    "        xc=self.ac2(self.bn2(self.conv2(xc)))\n",
    "        xc=self.ac3(self.bn3(self.conv3(xc)))\n",
    "        xc=self.ac4(self.bn4(self.conv4(xc)))\n",
    "        xc=self.ac5(self.bn5(self.conv5(xc)))\n",
    "        xc=self.ac6(self.bn6(self.conv6(xc)))\n",
    "        xc=self.ac7(self.bn7(self.conv7(xc)))\n",
    "        xc=self.ac8(self.bn8(self.conv8(xc)))\n",
    "        xc=self.ac9(xc+self.bn9(self.conv9(xc)))\n",
    "        xc=self.ac10(xc+self.bn10(self.conv10(xc)))\n",
    "        content=self.ac11(xc+self.bn11(self.conv11(xc)))   # bs,64,1000\n",
    "\n",
    "        \n",
    "        \n",
    "        xs=self.conv_sty_1(x)\n",
    "        xs=self.ac_sty_1(xs)\n",
    "        xs=self.conv_sty_2(xs)\n",
    "        xs=self.ac_sty_2(xs)\n",
    "        xs=self.conv_sty_3(xs)\n",
    "        xs=self.ac_sty_3(xs)\n",
    "        xs=self.conv_sty_4(xs)\n",
    "        xs=self.ac_sty_4(xs)\n",
    "        xs=self.conv_sty_5(xs)\n",
    "        xs=self.ac_sty_5(xs)\n",
    "        xs=self.conv_sty_6(xs)\n",
    "        xs=self.ac_sty_6(xs)\n",
    "        xs=self.pool_sty(xs)\n",
    "        style=self.fc_sty(xs)\n",
    "        \n",
    "        return content, style\n",
    "    \n",
    "    def decode(self,content, style):  # x1逆向操作，x2应用到自适应归一化层\n",
    "        \n",
    "        adain_params = self.mlp_1(style.view(style.size(0), -1))\n",
    "        adain_params = self.mlp_2(adain_params)\n",
    "        adain_params = self.mlp_3(adain_params)\n",
    "        adain_params = self.mlp_4(adain_params)\n",
    "        adain_params = self.mlp_5(adain_params)\n",
    "        self.assign_adain_params(adain_params, self.dec)\n",
    "        x=self.dec(content)\n",
    "        x=x.transpose(-1,-2)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def assign_adain_params(self, adain_params, model):\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                mean = adain_params[:, :m.num_features]\n",
    "                std = adain_params[:, m.num_features:2*m.num_features]\n",
    "                m.bias = mean.contiguous().view(-1)\n",
    "                m.weight = std.contiguous().view(-1)\n",
    "                if adain_params.size(1) > 2*m.num_features:\n",
    "                    adain_params = adain_params[:, 2*m.num_features:]\n",
    "        \n",
    "        \n",
    "    def get_num_adain_params(self,model):\n",
    "        num_adain_params = 0\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                num_adain_params += 2*m.num_features\n",
    "        return num_adain_params\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # reconstruct an image\n",
    "        content, style_fake = self.encode(images)\n",
    "        images_recon = self.decode(content, style_fake)\n",
    "        return images_recon\n",
    "    \n",
    "    \n",
    "    \n",
    "# 加载模型的流程定义\n",
    "\n",
    "def load_munit_model(model_path):\n",
    "\n",
    "    return MUNITModelOfNatVar(model_path).to(device)\n",
    "\n",
    "\n",
    "\n",
    "class MUNITModelOfNatVar(nn.Module):\n",
    "    def __init__(self, fname: str):\n",
    "\n",
    "        super(MUNITModelOfNatVar, self).__init__()\n",
    "\n",
    "        self._fname = fname\n",
    "        self._gen_A, self._gen_B = self.__load()\n",
    "        self.delta_dim = 8\n",
    "\n",
    "    def forward(self, x, delta):\n",
    "\n",
    "        orig_content, _ = self._gen_A.encode(x)\n",
    "        orig_content = orig_content.clone().detach().requires_grad_(False)\n",
    "        x_out = self._gen_B.decode(orig_content, delta)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def __load(self):\n",
    "        \"\"\"Load MUNIT model from file.\"\"\"\n",
    "\n",
    "        def load_munit(fname, letter):\n",
    "            gen = ECG_gen()\n",
    "            gen.load_state_dict(torch.load(fname)[letter])\n",
    "            return gen.eval()\n",
    "\n",
    "        gen_A = load_munit(self._fname, 'a')\n",
    "        gen_B = load_munit(self._fname, 'b')\n",
    "\n",
    "        return gen_A, gen_B     # original order\n",
    "        \n",
    "# 算法总模型定义\n",
    "\n",
    "\n",
    "class Algorithm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(Algorithm, self).__init__()\n",
    "\n",
    "    def update(self, in_data_, targets_, unlabeled=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ERM(Algorithm):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super(ERM, self).__init__()\n",
    "\n",
    "\n",
    "        self.network = generate_model(base_model='cnn_Ag',input_channels=1, num_classes=3,DG_method=None,domain_classes=6,distill=False)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(),lr=1e-3,weight_decay=0)\n",
    "\n",
    "    def update(self, in_data_, targets_, unlabeled=None):\n",
    "        \n",
    "        loss = F.cross_entropy(self.predict(in_data_), targets_)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return {'loss': loss.item()}\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class MBDG_Base(ERM):\n",
    "    def __init__(self, ):\n",
    "        super(MBDG_Base, self).__init__()\n",
    "    \n",
    "        self.G = load_munit_model('save_model_states/ABCLN_H_/gen_00600000.pt')\n",
    "\n",
    "    @staticmethod\n",
    "    def kl_div(dist1, dist2):\n",
    "        return F.kl_div(torch.log(dist2), dist1, reduction='batchmean')\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_images(self, images):\n",
    "        delta = torch.randn(images.size(0), self.G.delta_dim).to(device).requires_grad_(False)\n",
    "        return self.G(images, delta)\n",
    "\n",
    "    def calc_dist_reg(self, x, clean_output):\n",
    "        mb_images = self.generate_images(x)\n",
    "        mb_output = F.softmax(self.predict(mb_images), dim=1)\n",
    "        return self.kl_div(F.softmax(clean_output, dim=1), mb_output)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return x if x > 0 else torch.tensor(0).to(device)\n",
    "\n",
    "class MBDG(MBDG_Base):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super(MBDG, self).__init__()\n",
    "        self.dual_var = torch.tensor(1.0).to(device).requires_grad_(False)\n",
    "\n",
    "    def update(self, in_data_, targets_, unlabeled=None):\n",
    "\n",
    "        clean_output = self.predict(in_data_)\n",
    "        clean_loss = F.cross_entropy(clean_output, targets_)\n",
    "        dist_reg = self.calc_dist_reg(in_data_, clean_output)\n",
    "\n",
    "        loss = clean_loss + self.dual_var * dist_reg\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        const_unsat = dist_reg.detach() - 0.025\n",
    "        self.dual_var = self.relu(self.dual_var + 0.05 * const_unsat)\n",
    "\n",
    "        return {'loss': loss.item(), 'dist_reg': dist_reg.item(), 'dual_var': self.dual_var.item()}\n",
    "\n",
    "    \n",
    "    \n",
    "cls_model=MBDG().to(device)   \n",
    "cls_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0160df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data L_12\n",
      "train data size : 16824\n",
      "train label class :  [13511.  1158.  2155. 16824.     0.     0.     0.     0.]\n",
      "train data B_12\n",
      "train data size : 5501\n",
      "train label class :  [ 742.  859. 3900.    0. 5501.    0.    0.    0.]\n",
      "train data C_12\n",
      "train data size : 24194\n",
      "train label class :  [ 6719.  2039. 15436.     0.     0. 24194.     0.     0.]\n",
      "train data A_12\n",
      "train data size : 8274\n",
      "train label class :  [1393.  480. 6401.    0.    0.    0. 8274.    0.]\n",
      "train data N_12\n",
      "train data size : 8516\n",
      "train label class :  [1462. 1769. 5285.    0.    0.    0.    0. 8516.]\n",
      "total train data shape =  (63309, 1000, 12)\n",
      "total train label shape =  (63309, 8)\n",
      "total train data class =  [23827.  6305. 33177. 16824.  5501. 24194.  8274.  8516.]\n",
      "val data A_12\n",
      "val data size : 2069\n",
      "val label class :  [ 359.   90. 1620.]\n",
      "total val data shape =  (2069, 1000, 12)\n",
      "total val label shape =  (2069, 3)\n",
      "total val data class =  [ 359.   90. 1620.]\n",
      "val data B_12\n",
      "val data size : 1376\n",
      "val label class :  [176. 239. 961.]\n",
      "total val data shape =  (1376, 1000, 12)\n",
      "total val label shape =  (1376, 3)\n",
      "total val data class =  [176. 239. 961.]\n",
      "val data C_12\n",
      "val data size : 6049\n",
      "val label class :  [1485.  584. 3980.]\n",
      "total val data shape =  (6049, 1000, 12)\n",
      "total val label shape =  (6049, 3)\n",
      "total val data class =  [1485.  584. 3980.]\n",
      "val data N_12\n",
      "val data size : 2130\n",
      "val label class :  [ 364.  456. 1310.]\n",
      "total val data shape =  (2130, 1000, 12)\n",
      "total val label shape =  (2130, 3)\n",
      "total val data class =  [ 364.  456. 1310.]\n",
      "val data L_12\n",
      "val data size : 4206\n",
      "val label class :  [3237.  356.  613.]\n",
      "total val data shape =  (4206, 1000, 12)\n",
      "total val label shape =  (4206, 3)\n",
      "total val data class =  [3237.  356.  613.]\n",
      "test data H\n",
      "test data size : 25770\n",
      "test label class :  [13905.   675. 11190.]\n",
      "total test data shape =  (25770, 1000, 1)\n",
      "total test label shape =  (25770, 3)\n",
      "total test data class =  [13905.   675. 11190.]\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_data_list=['L_12','B_12','C_12','A_12','N_12']\n",
    "\n",
    "train_datasloader_instance=train_loader(dataname=train_data_list,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,smote=smote,maple=False,class_num=class_num)\n",
    "train_datasloader=train_datasloader_instance.loader()\n",
    " \n",
    "val_data_list_A=['A_12']\n",
    "val_data_list_B=['B_12']\n",
    "val_data_list_C=['C_12']\n",
    "val_data_list_N=['N_12']\n",
    "val_data_list_L=['L_12']\n",
    "# val_data_list_N=['N_12']\n",
    "\n",
    "\n",
    "val_datasloader_A=val_loader(dataname=val_data_list_A,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "val_datasloader_B=val_loader(dataname=val_data_list_B,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "val_datasloader_C=val_loader(dataname=val_data_list_C,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "val_datasloader_N=val_loader(dataname=val_data_list_N,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "val_datasloader_L=val_loader(dataname=val_data_list_L,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "# val_datasloader_N=val_loader(dataname=val_data_list_N,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "\n",
    "\n",
    "test_H_data_list=['H']\n",
    "test_H_datasloader=test_loader(dataname=test_H_data_list,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "# test_R_data_list=['R_3cls']\n",
    "# test_R_datasloader=test_loader(dataname=test_R_data_list,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da0798fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model=cnn_Ag\n",
      "base_model=cnn_Ag_teacher_self_distill_leadI\n",
      "ok\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [43:41<00:00,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0\n",
      "train_running_loss= 0.5731050976141226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0\n",
      "val_loss= 0.7968953251838684\n",
      "________________________________\n",
      "848 646 706 600 445 700 654 581\n",
      "epoch= 0\n",
      "val_loss= 0.7279904051260515\n",
      "________________________________\n",
      "882 680 757 650 468 781 702 653\n",
      "epoch= 0\n",
      "val_loss= 0.585635614891847\n",
      "________________________________\n",
      "913 784 821 762 644 903 740 720\n",
      "epoch= 0\n",
      "val_loss= 0.6040572699378518\n",
      "________________________________\n",
      "906 731 747 701 569 805 729 700\n",
      "epoch= 0\n",
      "val_loss= 0.4674198528130849\n",
      "________________________________\n",
      "866 721 728 724 897 806 467 822\n",
      "epoch= 0\n",
      "test_loss= 0.6101057128150864\n",
      "________________________________\n",
      "819 684 662 656 767 646 557 690\n",
      "3437\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [45:00<00:00,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1\n",
      "train_running_loss= 0.5268618901716624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1\n",
      "val_loss= 0.611605258548961\n",
      "________________________________\n",
      "824 716 683 664 484 715 793 708\n",
      "epoch= 1\n",
      "val_loss= 0.5960132100365378\n",
      "________________________________\n",
      "878 693 696 665 460 749 786 716\n",
      "epoch= 1\n",
      "val_loss= 0.44731941632926464\n",
      "________________________________\n",
      "919 806 776 780 662 856 823 779\n",
      "epoch= 1\n",
      "val_loss= 0.42885353635339174\n",
      "________________________________\n",
      "957 854 757 790 728 769 873 831\n",
      "epoch= 1\n",
      "val_loss= 0.4922701066190546\n",
      "________________________________\n",
      "901 777 732 730 895 756 540 816\n",
      "epoch= 1\n",
      "test_loss= 0.6328994563900598\n",
      "________________________________\n",
      "837 744 650 672 590 777 650 626\n",
      "3629\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [45:01<00:00,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2\n",
      "train_running_loss= 0.5767998856498568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2\n",
      "val_loss= 0.4548429794171277\n",
      "________________________________\n",
      "858 660 731 690 528 680 863 791\n",
      "epoch= 2\n",
      "val_loss= 0.4739519249309193\n",
      "________________________________\n",
      "896 717 766 737 515 844 854 802\n",
      "epoch= 2\n",
      "val_loss= 0.3831798179696004\n",
      "________________________________\n",
      "927 813 842 826 679 932 867 827\n",
      "epoch= 2\n",
      "val_loss= 0.3080082711051492\n",
      "________________________________\n",
      "973 910 806 841 717 896 911 881\n",
      "epoch= 2\n",
      "val_loss= 0.6364298209999547\n",
      "________________________________\n",
      "885 699 786 707 816 831 472 735\n",
      "epoch= 2\n",
      "test_loss= 0.80475648322908\n",
      "________________________________\n",
      "848 693 681 604 348 825 640 545\n",
      "3801\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [45:11<00:00,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 3\n",
      "train_running_loss= 0.5106310187949699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 3\n",
      "val_loss= 0.5351078966084648\n",
      "________________________________\n",
      "854 731 726 714 541 753 848 775\n",
      "epoch= 3\n",
      "val_loss= 0.5937351990829814\n",
      "________________________________\n",
      "894 711 730 689 513 754 801 737\n",
      "epoch= 3\n",
      "val_loss= 0.4018629516164462\n",
      "________________________________\n",
      "941 824 830 818 727 876 853 818\n",
      "epoch= 3\n",
      "val_loss= 0.4301800499944126\n",
      "________________________________\n",
      "952 860 773 801 784 741 877 838\n",
      "epoch= 3\n",
      "val_loss= 0.49984177314873895\n",
      "________________________________\n",
      "891 733 715 706 877 755 485 791\n",
      "epoch= 3\n",
      "test_loss= 0.5312765169851851\n",
      "________________________________\n",
      "877 799 668 714 779 655 708 746\n",
      "3728\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [44:48<00:00,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4\n",
      "train_running_loss= 0.5075725476570617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4\n",
      "val_loss= 0.4489851225824917\n",
      "________________________________\n",
      "868 760 734 739 561 783 873 806\n",
      "epoch= 4\n",
      "val_loss= 0.4736874076453122\n",
      "________________________________\n",
      "905 721 766 728 524 825 835 778\n",
      "epoch= 4\n",
      "val_loss= 0.3429399337619543\n",
      "________________________________\n",
      "950 850 840 842 732 917 877 842\n",
      "epoch= 4\n",
      "val_loss= 0.3680661671301898\n",
      "________________________________\n",
      "966 891 787 828 751 835 897 863\n",
      "epoch= 4\n",
      "val_loss= 0.5529003143310547\n",
      "________________________________\n",
      "872 742 736 720 852 849 459 770\n",
      "epoch= 4\n",
      "test_loss= 0.6025333882558463\n",
      "________________________________\n",
      "872 761 683 695 598 811 675 645\n",
      "3857\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:44<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 5\n",
      "train_running_loss= 0.4401640503686031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 5\n",
      "val_loss= 0.487492619191899\n",
      "________________________________\n",
      "881 693 782 721 578 736 848 780\n",
      "epoch= 5\n",
      "val_loss= 0.4745794345032085\n",
      "________________________________\n",
      "917 730 822 754 562 862 840 793\n",
      "epoch= 5\n",
      "val_loss= 0.3148668045178056\n",
      "________________________________\n",
      "957 854 902 869 764 963 881 855\n",
      "epoch= 5\n",
      "val_loss= 0.3032631251741858\n",
      "________________________________\n",
      "968 879 871 874 801 907 914 892\n",
      "epoch= 5\n",
      "val_loss= 0.44862178509885614\n",
      "________________________________\n",
      "909 741 809 762 878 880 528 811\n",
      "epoch= 5\n",
      "test_loss= 0.557419156526575\n",
      "________________________________\n",
      "888 768 779 760 671 909 701 693\n",
      "3980\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:21<00:00,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6\n",
      "train_running_loss= 0.4033604155213463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6\n",
      "val_loss= 0.47494585899745717\n",
      "________________________________\n",
      "880 707 776 732 582 751 862 797\n",
      "epoch= 6\n",
      "val_loss= 0.4760541997172616\n",
      "________________________________\n",
      "920 723 820 748 562 849 833 786\n",
      "epoch= 6\n",
      "val_loss= 0.3081559566780925\n",
      "________________________________\n",
      "956 851 893 866 755 961 882 854\n",
      "epoch= 6\n",
      "val_loss= 0.2674968041041318\n",
      "________________________________\n",
      "973 903 880 891 841 905 926 908\n",
      "epoch= 6\n",
      "val_loss= 0.5608053378986589\n",
      "________________________________\n",
      "903 708 803 731 847 855 493 771\n",
      "epoch= 6\n",
      "test_loss= 0.5375271795114668\n",
      "________________________________\n",
      "886 743 804 765 730 847 718 727\n",
      "3968\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:24<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 7\n",
      "train_running_loss= 0.44844710075375416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 7\n",
      "val_loss= 0.4791397802969989\n",
      "________________________________\n",
      "874 716 760 726 567 757 853 784\n",
      "epoch= 7\n",
      "val_loss= 0.4266265061768619\n",
      "________________________________\n",
      "922 745 802 763 566 865 859 812\n",
      "epoch= 7\n",
      "val_loss= 0.321985259031256\n",
      "________________________________\n",
      "951 853 886 864 740 974 877 848\n",
      "epoch= 7\n",
      "val_loss= 0.280924536726054\n",
      "________________________________\n",
      "975 904 870 886 831 899 926 905\n",
      "epoch= 7\n",
      "val_loss= 0.34237290693051886\n",
      "________________________________\n",
      "918 798 813 805 925 884 605 872\n",
      "epoch= 7\n",
      "test_loss= 0.568950013211458\n",
      "________________________________\n",
      "893 777 752 741 624 904 696 670\n",
      "4044\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [42:36<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 8\n",
      "train_running_loss= 0.44978476937522277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 8\n",
      "val_loss= 0.4882846316870521\n",
      "________________________________\n",
      "876 712 752 714 575 721 848 778\n",
      "epoch= 8\n",
      "val_loss= 0.43973250822587445\n",
      "________________________________\n",
      "919 744 791 754 571 839 852 802\n",
      "epoch= 8\n",
      "val_loss= 0.3311400441452861\n",
      "________________________________\n",
      "952 856 879 859 751 951 874 845\n",
      "epoch= 8\n",
      "val_loss= 0.3138708831632839\n",
      "________________________________\n",
      "973 915 847 874 857 844 922 896\n",
      "epoch= 8\n",
      "val_loss= 0.35390963518258295\n",
      "________________________________\n",
      "927 809 801 802 926 873 608 871\n",
      "epoch= 8\n",
      "test_loss= 0.5566810298674177\n",
      "________________________________\n",
      "902 793 733 745 677 844 714 700\n",
      "4003\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [43:13<00:00,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 9\n",
      "train_running_loss= 0.4651963788212815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 9\n",
      "val_loss= 0.4535716386402355\n",
      "________________________________\n",
      "874 689 730 706 517 719 881 813\n",
      "epoch= 9\n",
      "val_loss= 0.4274515780535611\n",
      "________________________________\n",
      "918 743 767 754 520 865 878 829\n",
      "epoch= 9\n",
      "val_loss= 0.30519859430690605\n",
      "________________________________\n",
      "956 856 854 854 709 958 894 858\n",
      "epoch= 9\n",
      "val_loss= 0.30187410817426796\n",
      "________________________________\n",
      "971 894 824 849 743 896 908 882\n",
      "epoch= 9\n",
      "val_loss= 0.4965333983753667\n",
      "________________________________\n",
      "908 738 813 758 866 890 518 797\n",
      "epoch= 9\n",
      "test_loss= 0.6917024502659789\n",
      "________________________________\n",
      "884 715 722 663 479 848 662 598\n",
      "3921\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [42:42<00:00,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10\n",
      "train_running_loss= 0.4100747748470614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10\n",
      "val_loss= 0.49968500698313995\n",
      "________________________________\n",
      "874 722 767 733 577 764 859 792\n",
      "epoch= 10\n",
      "val_loss= 0.44934027032418683\n",
      "________________________________\n",
      "918 740 800 758 541 880 854 806\n",
      "epoch= 10\n",
      "val_loss= 0.3207554304972291\n",
      "________________________________\n",
      "953 853 878 863 739 966 884 853\n",
      "epoch= 10\n",
      "val_loss= 0.2600244651822483\n",
      "________________________________\n",
      "978 915 882 897 862 896 932 913\n",
      "epoch= 10\n",
      "val_loss= 0.3422764291365941\n",
      "________________________________\n",
      "927 794 816 804 923 888 602 870\n",
      "epoch= 10\n",
      "test_loss= 0.6198059481264341\n",
      "________________________________\n",
      "881 737 737 703 549 892 668 626\n",
      "4055\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:17<00:00,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 11\n",
      "train_running_loss= 0.42095690707819067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 11\n",
      "val_loss= 0.49581894103218527\n",
      "________________________________\n",
      "867 689 750 714 561 715 868 799\n",
      "epoch= 11\n",
      "val_loss= 0.463405812328512\n",
      "________________________________\n",
      "919 723 792 748 530 863 850 801\n",
      "epoch= 11\n",
      "val_loss= 0.2986158262938261\n",
      "________________________________\n",
      "956 863 880 871 750 967 897 867\n",
      "epoch= 11\n",
      "val_loss= 0.2436185707064236\n",
      "________________________________\n",
      "977 904 883 893 851 901 928 909\n",
      "epoch= 11\n",
      "val_loss= 0.3843203340515946\n",
      "________________________________\n",
      "925 769 837 796 906 891 590 849\n",
      "epoch= 11\n",
      "test_loss= 0.6290348836100927\n",
      "________________________________\n",
      "857 710 738 701 575 873 655 627\n",
      "4022\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:52<00:00,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 12\n",
      "train_running_loss= 0.4366338277793954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 12\n",
      "val_loss= 0.4445904879009022\n",
      "________________________________\n",
      "894 730 782 750 609 761 878 817\n",
      "epoch= 12\n",
      "val_loss= 0.42339973016218707\n",
      "________________________________\n",
      "928 747 817 773 583 870 865 821\n",
      "epoch= 12\n",
      "val_loss= 0.29549363752206165\n",
      "________________________________\n",
      "959 865 896 876 763 974 890 863\n",
      "epoch= 12\n",
      "val_loss= 0.2577345117050059\n",
      "________________________________\n",
      "976 919 879 897 853 906 933 915\n",
      "epoch= 12\n",
      "val_loss= 0.45898773363142303\n",
      "________________________________\n",
      "925 746 820 769 879 890 537 813\n",
      "epoch= 12\n",
      "test_loss= 0.5854779658931317\n",
      "________________________________\n",
      "897 751 781 743 641 886 702 680\n",
      "4065\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:51<00:00,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 13\n",
      "train_running_loss= 0.4591458425165651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 13\n",
      "val_loss= 0.4690589519108043\n",
      "________________________________\n",
      "894 707 796 736 593 756 859 794\n",
      "epoch= 13\n",
      "val_loss= 0.45764445716684515\n",
      "________________________________\n",
      "921 731 809 755 549 870 847 800\n",
      "epoch= 13\n",
      "val_loss= 0.33276493816326064\n",
      "________________________________\n",
      "956 848 899 865 759 958 878 851\n",
      "epoch= 13\n",
      "val_loss= 0.2219617419383105\n",
      "________________________________\n",
      "982 925 898 911 879 913 941 925\n",
      "epoch= 13\n",
      "val_loss= 0.3340309393225294\n",
      "________________________________\n",
      "932 805 833 818 929 892 634 880\n",
      "epoch= 13\n",
      "test_loss= 0.5540679852561196\n",
      "________________________________\n",
      "899 778 782 762 664 909 712 696\n",
      "4085\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:06<00:00,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 14\n",
      "train_running_loss= 0.4352026438129467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 14\n",
      "val_loss= 0.45435667563887205\n",
      "________________________________\n",
      "892 722 791 744 616 746 869 807\n",
      "epoch= 14\n",
      "val_loss= 0.4457071775739843\n",
      "________________________________\n",
      "926 743 828 765 582 865 849 804\n",
      "epoch= 14\n",
      "val_loss= 0.2852462725713849\n",
      "________________________________\n",
      "964 865 907 879 781 964 892 867\n",
      "epoch= 14\n",
      "val_loss= 0.26249489626463723\n",
      "________________________________\n",
      "974 904 888 895 859 898 928 910\n",
      "epoch= 14\n",
      "val_loss= 0.472699196952762\n",
      "________________________________\n",
      "920 753 807 764 875 885 531 806\n",
      "epoch= 14\n",
      "test_loss= 0.4912784509729631\n",
      "________________________________\n",
      "899 800 818 808 788 891 746 772\n",
      "4047\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:45<00:00,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 15\n",
      "train_running_loss= 0.36127391799480035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 15\n",
      "val_loss= 0.45094507231431846\n",
      "________________________________\n",
      "879 728 758 739 580 760 876 812\n",
      "epoch= 15\n",
      "val_loss= 0.46315240318124945\n",
      "________________________________\n",
      "920 733 789 749 540 858 849 799\n",
      "epoch= 15\n",
      "val_loss= 0.29267201013863087\n",
      "________________________________\n",
      "958 869 879 873 750 974 895 864\n",
      "epoch= 15\n",
      "val_loss= 0.2571014867109411\n",
      "________________________________\n",
      "979 914 879 894 851 901 931 912\n",
      "epoch= 15\n",
      "val_loss= 0.36818278016466083\n",
      "________________________________\n",
      "922 798 812 801 915 892 596 859\n",
      "epoch= 15\n",
      "test_loss= 0.5290715301981067\n",
      "________________________________\n",
      "881 777 787 778 718 911 704 717\n",
      "4056\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:45<00:00,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 16\n",
      "train_running_loss= 0.39580026672631774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 16\n",
      "val_loss= 0.4539749429506414\n",
      "________________________________\n",
      "878 713 784 741 585 767 871 807\n",
      "epoch= 16\n",
      "val_loss= 0.44405716928568756\n",
      "________________________________\n",
      "921 733 812 759 563 862 851 805\n",
      "epoch= 16\n",
      "val_loss= 0.2775641679763794\n",
      "________________________________\n",
      "962 867 897 880 771 968 899 872\n",
      "epoch= 16\n",
      "val_loss= 0.24978864105308757\n",
      "________________________________\n",
      "977 914 885 899 851 913 933 915\n",
      "epoch= 16\n",
      "val_loss= 0.37834981580575305\n",
      "________________________________\n",
      "919 771 824 794 912 883 587 856\n",
      "epoch= 16\n",
      "test_loss= 0.5242879673986152\n",
      "________________________________\n",
      "888 775 805 785 735 904 717 731\n",
      "4073\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:48<00:00,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 17\n",
      "train_running_loss= 0.45488030886956676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 17\n",
      "val_loss= 0.4546886472141041\n",
      "________________________________\n",
      "884 734 773 749 589 780 878 816\n",
      "epoch= 17\n",
      "val_loss= 0.42711837183345447\n",
      "________________________________\n",
      "924 743 792 762 564 856 866 818\n",
      "epoch= 17\n",
      "val_loss= 0.29226004735877115\n",
      "________________________________\n",
      "959 862 895 876 766 966 895 867\n",
      "epoch= 17\n",
      "val_loss= 0.27053626845864687\n",
      "________________________________\n",
      "975 914 861 885 827 900 926 905\n",
      "epoch= 17\n",
      "val_loss= 0.3543539160128796\n",
      "________________________________\n",
      "932 776 829 798 918 868 609 862\n",
      "epoch= 17\n",
      "test_loss= 0.592449686904945\n",
      "________________________________\n",
      "893 742 753 720 610 861 690 661\n",
      "4070\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:37<00:00,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 18\n",
      "train_running_loss= 0.40147513666027007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 18\n",
      "val_loss= 0.47683202869751873\n",
      "________________________________\n",
      "882 699 799 735 609 730 866 802\n",
      "epoch= 18\n",
      "val_loss= 0.42962058023972943\n",
      "________________________________\n",
      "931 750 827 773 595 864 859 814\n",
      "epoch= 18\n",
      "val_loss= 0.2966214610884587\n",
      "________________________________\n",
      "956 855 899 871 762 966 886 859\n",
      "epoch= 18\n",
      "val_loss= 0.24711075951071346\n",
      "________________________________\n",
      "975 905 890 897 850 911 931 913\n",
      "epoch= 18\n",
      "val_loss= 0.37632081183520233\n",
      "________________________________\n",
      "915 774 813 792 916 886 574 859\n",
      "epoch= 18\n",
      "test_loss= 0.4890768706503481\n",
      "________________________________\n",
      "887 798 818 808 794 900 729 769\n",
      "4068\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:02<00:00,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 19\n",
      "train_running_loss= 0.4288809986956424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 19\n",
      "val_loss= 0.4751683648894815\n",
      "________________________________\n",
      "872 708 766 728 566 757 861 792\n",
      "epoch= 19\n",
      "val_loss= 0.45199076154015283\n",
      "________________________________\n",
      "920 728 795 746 538 860 841 791\n",
      "epoch= 19\n",
      "val_loss= 0.264183952473104\n",
      "________________________________\n",
      "966 877 900 887 782 974 906 880\n",
      "epoch= 19\n",
      "val_loss= 0.23605335109374104\n",
      "________________________________\n",
      "981 926 883 903 867 905 936 918\n",
      "epoch= 19\n",
      "val_loss= 0.3286687790444403\n",
      "________________________________\n",
      "934 820 840 828 930 911 644 883\n",
      "epoch= 19\n",
      "test_loss= 0.5819836834279617\n",
      "________________________________\n",
      "871 747 736 723 606 892 670 648\n",
      "4092\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:36<00:00,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 20\n",
      "train_running_loss= 0.4248262885112531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 20\n",
      "val_loss= 0.4718462737167583\n",
      "________________________________\n",
      "880 719 770 737 577 765 868 801\n",
      "epoch= 20\n",
      "val_loss= 0.43994633717970416\n",
      "________________________________\n",
      "922 733 797 756 553 859 855 807\n",
      "epoch= 20\n",
      "val_loss= 0.28320919411877793\n",
      "________________________________\n",
      "961 869 894 880 766 975 899 871\n",
      "epoch= 20\n",
      "val_loss= 0.227952959782937\n",
      "________________________________\n",
      "983 936 893 913 882 914 942 927\n",
      "epoch= 20\n",
      "val_loss= 0.3273411497022166\n",
      "________________________________\n",
      "935 804 835 818 927 895 632 878\n",
      "epoch= 20\n",
      "test_loss= 0.5715635457251331\n",
      "________________________________\n",
      "876 758 772 757 685 895 692 694\n",
      "4104\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [42:31<00:00,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 21\n",
      "train_running_loss= 0.3827872883614576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 21\n",
      "val_loss= 0.4578915115664987\n",
      "________________________________\n",
      "882 742 778 750 592 789 869 805\n",
      "epoch= 21\n",
      "val_loss= 0.4477210749279369\n",
      "________________________________\n",
      "927 740 808 756 585 834 849 799\n",
      "epoch= 21\n",
      "val_loss= 0.28818345349282026\n",
      "________________________________\n",
      "960 872 894 878 772 969 894 867\n",
      "epoch= 21\n",
      "val_loss= 0.25496116543517394\n",
      "________________________________\n",
      "977 914 885 898 866 894 933 914\n",
      "epoch= 21\n",
      "val_loss= 0.3364085035793709\n",
      "________________________________\n",
      "929 815 813 812 927 870 638 876\n",
      "epoch= 21\n",
      "test_loss= 0.5228123421421146\n",
      "________________________________\n",
      "895 800 757 773 746 846 727 739\n",
      "4094\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [42:55<00:00,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 22\n",
      "train_running_loss= 0.37241504414376275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 22\n",
      "val_loss= 0.4299645160927492\n",
      "________________________________\n",
      "876 747 731 739 555 768 894 831\n",
      "epoch= 22\n",
      "val_loss= 0.4106750244444067\n",
      "________________________________\n",
      "923 752 754 753 530 852 876 826\n",
      "epoch= 22\n",
      "val_loss= 0.28320370086779195\n",
      "________________________________\n",
      "963 881 851 863 717 969 903 868\n",
      "epoch= 22\n",
      "val_loss= 0.2624901647076887\n",
      "________________________________\n",
      "979 927 857 887 849 884 928 907\n",
      "epoch= 22\n",
      "val_loss= 0.4247418262741782\n",
      "________________________________\n",
      "914 775 804 778 892 884 558 829\n",
      "epoch= 22\n",
      "test_loss= 0.7616372194030497\n",
      "________________________________\n",
      "883 748 679 628 345 894 646 550\n",
      "4020\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [42:59<00:00,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 23\n",
      "train_running_loss= 0.35328602588232333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 23\n",
      "val_loss= 0.46604379485635195\n",
      "________________________________\n",
      "876 743 741 739 557 784 876 810\n",
      "epoch= 23\n",
      "val_loss= 0.42906092784621497\n",
      "________________________________\n",
      "925 747 786 761 570 846 866 818\n",
      "epoch= 23\n",
      "val_loss= 0.2696375483646989\n",
      "________________________________\n",
      "965 882 893 887 780 973 910 883\n",
      "epoch= 23\n",
      "val_loss= 0.25459568991380577\n",
      "________________________________\n",
      "981 921 879 898 859 901 934 915\n",
      "epoch= 23\n",
      "val_loss= 0.38316480589635443\n",
      "________________________________\n",
      "929 782 819 792 902 899 576 843\n",
      "epoch= 23\n",
      "test_loss= 0.6222895549075438\n",
      "________________________________\n",
      "892 768 762 740 616 908 695 667\n",
      "4077\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [43:40<00:00,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 24\n",
      "train_running_loss= 0.3967108930275877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 24\n",
      "val_loss= 0.43946078244377584\n",
      "________________________________\n",
      "880 727 710 718 509 761 884 816\n",
      "epoch= 24\n",
      "val_loss= 0.4067199609496377\n",
      "________________________________\n",
      "924 760 755 757 532 855 884 835\n",
      "epoch= 24\n",
      "val_loss= 0.30181305420895416\n",
      "________________________________\n",
      "958 868 845 854 694 970 896 858\n",
      "epoch= 24\n",
      "val_loss= 0.28167159680057974\n",
      "________________________________\n",
      "973 905 841 868 782 906 918 894\n",
      "epoch= 24\n",
      "val_loss= 0.38031279679500696\n",
      "________________________________\n",
      "921 781 813 791 906 887 581 848\n",
      "epoch= 24\n",
      "test_loss= 0.6974921754681238\n",
      "________________________________\n",
      "886 731 676 612 309 888 638 536\n",
      "3988\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [46:36<00:00,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 25\n",
      "train_running_loss= 0.39485160056217655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 25\n",
      "val_loss= 0.4413673369323506\n",
      "________________________________\n",
      "882 755 726 735 587 733 884 821\n",
      "epoch= 25\n",
      "val_loss= 0.4028996689753099\n",
      "________________________________\n",
      "930 762 788 770 574 860 876 828\n",
      "epoch= 25\n",
      "val_loss= 0.2584494131927689\n",
      "________________________________\n",
      "968 887 891 888 786 968 910 883\n",
      "epoch= 25\n",
      "val_loss= 0.3074089455253938\n",
      "________________________________\n",
      "972 911 841 870 820 872 918 893\n",
      "epoch= 25\n",
      "val_loss= 0.4087580054095297\n",
      "________________________________\n",
      "929 796 806 791 910 871 592 849\n",
      "epoch= 25\n",
      "test_loss= 0.6225372005512219\n",
      "________________________________\n",
      "889 762 727 707 549 893 678 632\n",
      "4054\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [44:29<00:00,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 26\n",
      "train_running_loss= 0.39769821831697766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 26\n",
      "val_loss= 0.4370422328219694\n",
      "________________________________\n",
      "882 753 750 751 581 782 890 828\n",
      "epoch= 26\n",
      "val_loss= 0.4013628742911599\n",
      "________________________________\n",
      "930 762 767 765 543 867 884 836\n",
      "epoch= 26\n",
      "val_loss= 0.2832267476866643\n",
      "________________________________\n",
      "960 872 867 869 732 976 900 867\n",
      "epoch= 26\n",
      "val_loss= 0.24737243880243862\n",
      "________________________________\n",
      "980 924 876 898 846 913 935 916\n",
      "epoch= 26\n",
      "val_loss= 0.3516346708391652\n",
      "________________________________\n",
      "927 806 824 812 923 893 621 871\n",
      "epoch= 26\n",
      "test_loss= 0.660914660680412\n",
      "________________________________\n",
      "888 752 702 664 435 901 657 582\n",
      "4095\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [42:45<00:00,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 27\n",
      "train_running_loss= 0.4076819438936496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 27\n",
      "val_loss= 0.45863378398558674\n",
      "________________________________\n",
      "874 702 742 721 546 740 877 811\n",
      "epoch= 27\n",
      "val_loss= 0.42653496157039295\n",
      "________________________________\n",
      "925 743 797 765 561 866 869 823\n",
      "epoch= 27\n",
      "val_loss= 0.2628624200200041\n",
      "________________________________\n",
      "965 880 887 883 768 972 910 881\n",
      "epoch= 27\n",
      "val_loss= 0.2597108895287794\n",
      "________________________________\n",
      "977 919 866 889 831 908 929 909\n",
      "epoch= 27\n",
      "val_loss= 0.4564653860800194\n",
      "________________________________\n",
      "921 762 817 776 886 891 550 821\n",
      "epoch= 27\n",
      "test_loss= 0.6674115106020824\n",
      "________________________________\n",
      "887 739 725 672 456 898 662 592\n",
      "4034\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [43:24<00:00,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 28\n",
      "train_running_loss= 0.3928291383333126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 28\n",
      "val_loss= 0.4541604624075048\n",
      "________________________________\n",
      "868 717 717 717 519 753 878 809\n",
      "epoch= 28\n",
      "val_loss= 0.42733701250769873\n",
      "________________________________\n",
      "919 736 759 746 504 866 869 818\n",
      "epoch= 28\n",
      "val_loss= 0.27792809686313075\n",
      "________________________________\n",
      "962 877 861 867 724 976 903 869\n",
      "epoch= 28\n",
      "val_loss= 0.2539331255590214\n",
      "________________________________\n",
      "978 915 872 892 840 905 930 910\n",
      "epoch= 28\n",
      "val_loss= 0.3553857058286667\n",
      "________________________________\n",
      "930 793 821 805 919 893 602 865\n",
      "epoch= 28\n",
      "test_loss= 0.6172174306789248\n",
      "________________________________\n",
      "889 750 737 701 523 907 672 620\n",
      "4027\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [42:53<00:00,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 29\n",
      "train_running_loss= 0.36563167891545084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 29\n",
      "val_loss= 0.49546000010827007\n",
      "________________________________\n",
      "885 736 789 755 610 776 878 817\n",
      "epoch= 29\n",
      "val_loss= 0.49065253138542175\n",
      "________________________________\n",
      "925 742 817 763 574 864 852 806\n",
      "epoch= 29\n",
      "val_loss= 0.28869469743222\n",
      "________________________________\n",
      "962 875 900 886 775 980 902 876\n",
      "epoch= 29\n",
      "val_loss= 0.2173573103021173\n",
      "________________________________\n",
      "985 942 899 919 896 915 946 931\n",
      "epoch= 29\n",
      "val_loss= 0.34316415226820746\n",
      "________________________________\n",
      "933 801 830 813 920 901 616 869\n",
      "epoch= 29\n",
      "test_loss= 0.5219258437357327\n",
      "________________________________\n",
      "894 784 797 785 726 909 720 728\n",
      "4136\n",
      "max_epoch= 29\n",
      "max_val_f1_all= 4136\n",
      "4136 29\n",
      "_____________H_____________::\n",
      "['894', '784', '797', '785', '726', '909', '720', '728']\n",
      "___________________________::\n"
     ]
    }
   ],
   "source": [
    "# 更改加载的教师模型字典：\n",
    "# 加载5项不同的教师模型\n",
    "# 开始训练流程\n",
    "Teacher_queue=TeacherQueue(max_size=CRL_queue_len)\n",
    "student_model=generate_model(base_model=base_model_student,input_channels=2, num_classes=class_num,DG_method=DG_method_student,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model=generate_model(base_model=base_model_teacher,input_channels=12, num_classes=class_num,DG_method=DG_method_teacher,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model.load_state_dict(torch.load('/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/1711095663_ABCLN_H_proposed_cnnAg_zong_f1.pth'))\n",
    "teacher_model.eval()\n",
    "print('ok')\n",
    "optimizer=generate_optimizer(base_optimizer=base_optimizer,model_parameters=student_model.parameters(),SAM_flag=False,weight_decay=1e-4)\n",
    "\n",
    "mdst_path='/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/'\n",
    "mdst_name=str(int(time.time()))+'_ABCLN_H_zishiying_proposed_xuesheng_cnnAg_zong_f1.pth'\n",
    "\n",
    "max_val_f1_all=0\n",
    "max_epoch=0\n",
    "max_test_H_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "# max_test_R_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    print('_____________________________________')\n",
    "    print(' ')\n",
    "    time.sleep(0.1)\n",
    "    train_student_one_epoch(epoch,warm_epoch,total_epoch,student_model,teacher_model, \\\n",
    "                            max_lr,min_lr,loss_fn_clf,loss_fn_tslb,optimizer, \\\n",
    "                            feature_contrastive_loss_fn, \\\n",
    "                            Teacher_queue,base_optimizer, \\\n",
    "                            train_datasloader,device,GRL_loss_beta,GAN_aug='0211',GAN_model=cls_model)\n",
    "    \n",
    "    auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_A, device)\n",
    "    print(auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_B, device)\n",
    "    print(auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_C, device)\n",
    "    print(auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_N, device)\n",
    "    print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_L, device)\n",
    "    print(auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc)\n",
    "#     auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           val_datasloader_N, device)\n",
    "#     print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "                                         \n",
    "    H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          test_H_datasloader, device)\n",
    "    print(H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc)\n",
    "\n",
    "#     R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           test_R_datasloader, device)\n",
    "#     print(R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc)\n",
    "    \n",
    "    f1_val_all=int(f1_val_A)+int(f1_val_B)+int(f1_val_C)+int(f1_val_N)+int(f1_val_L)\n",
    "    \n",
    "    if int(f1_val_all)>max_val_f1_all:\n",
    "        max_val_f1_all=int(f1_val_all)\n",
    "        max_epoch=epoch\n",
    "        max_model_dict=student_model.state_dict()\n",
    "        # 指定你想要保存模型参数的路径  \n",
    "        torch.save(max_model_dict, os.path.join(mdst_path, mdst_name))\n",
    "        # 使用torch.save()保存模型参数  \n",
    "        \n",
    "        max_test_H_dict['auc']=H_auc\n",
    "        max_test_H_dict['pre']=H_pre\n",
    "        max_test_H_dict['rec']=H_rec\n",
    "        max_test_H_dict['f1_test']=H_f1_test\n",
    "        max_test_H_dict['f1_test_normal']=H_f1_test_normal\n",
    "        max_test_H_dict['f1_test_AF']=H_f1_test_AF\n",
    "        max_test_H_dict['f1_test_Other']=H_f1_test_Other\n",
    "        max_test_H_dict['acc']=H_acc\n",
    "        \n",
    "    print(f1_val_all)\n",
    "        \n",
    "print('max_epoch=',max_epoch)\n",
    "print('max_val_f1_all=',max_val_f1_all)\n",
    "        \n",
    "print (max_val_f1_all,max_epoch)\n",
    "print('_____________H_____________::')\n",
    "print(list(max_test_H_dict.values()))\n",
    "print('___________________________::')                                         \n",
    "\n",
    "\n",
    "\n",
    "# 写入csv\n",
    "\n",
    "header=[\n",
    "        'train\\\\test\\t',\n",
    "        'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "#         'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "\n",
    "        # round\n",
    "\n",
    "results=[\n",
    "        [\n",
    "        'H',\n",
    "        max_test_H_dict['auc'],\n",
    "        max_test_H_dict['pre'],\n",
    "        max_test_H_dict['rec'],\n",
    "        max_test_H_dict['f1_test'],\n",
    "        max_test_H_dict['f1_test_normal'],\n",
    "        max_test_H_dict['f1_test_AF'],\n",
    "        max_test_H_dict['f1_test_Other'],\n",
    "        max_test_H_dict['acc']+'\\t',\n",
    "         ],\n",
    "        ]\n",
    "\n",
    "with open('log/proposed_ABCLN_H_zishiying_600000_20240321_total_f1.csv','a',encoding='utf-8')as file_obj:\n",
    "    writer=csv.writer(file_obj)\n",
    "    writer.writerow(header)\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9caf92ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model=cnn_Ag\n",
      "base_model=cnn_Ag_teacher_self_distill_leadI\n",
      "ok\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [42:50<00:00,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0\n",
      "train_running_loss= 0.5313279812962253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0\n",
      "val_loss= 0.6829633151783663\n",
      "________________________________\n",
      "875 624 780 640 515 661 744 669\n",
      "epoch= 0\n",
      "val_loss= 0.7267152883789756\n",
      "________________________________\n",
      "889 670 796 669 485 804 717 677\n",
      "epoch= 0\n",
      "val_loss= 0.5162777031461397\n",
      "________________________________\n",
      "933 769 858 779 679 883 774 753\n",
      "epoch= 0\n",
      "val_loss= 0.29606813455329223\n",
      "________________________________\n",
      "968 878 887 882 857 871 919 898\n",
      "epoch= 0\n",
      "val_loss= 0.4922094281875726\n",
      "________________________________\n",
      "898 717 790 747 892 825 523 821\n",
      "epoch= 0\n",
      "test_loss= 0.5251798938111504\n",
      "________________________________\n",
      "874 749 765 750 806 760 685 758\n",
      "3717\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [44:55<00:00,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1\n",
      "train_running_loss= 0.6047725834413046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1\n",
      "val_loss= 0.4407374981571646\n",
      "________________________________\n",
      "887 734 772 743 604 754 870 807\n",
      "epoch= 1\n",
      "val_loss= 0.4923957613381473\n",
      "________________________________\n",
      "903 717 773 726 519 834 824 770\n",
      "epoch= 1\n",
      "val_loss= 0.36271905712783337\n",
      "________________________________\n",
      "945 838 864 842 733 933 862 830\n",
      "epoch= 1\n",
      "val_loss= 0.3098740831894033\n",
      "________________________________\n",
      "976 917 845 875 845 860 921 897\n",
      "epoch= 1\n",
      "val_loss= 0.8959999879201254\n",
      "________________________________\n",
      "898 707 749 661 729 837 419 645\n",
      "epoch= 1\n",
      "test_loss= 0.5123838357406088\n",
      "________________________________\n",
      "894 804 772 787 788 835 737 767\n",
      "3847\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [43:31<00:00,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2\n",
      "train_running_loss= 0.5552355856354113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2\n",
      "val_loss= 0.4832720861715429\n",
      "________________________________\n",
      "875 727 759 726 561 772 847 776\n",
      "epoch= 2\n",
      "val_loss= 0.549612741578709\n",
      "________________________________\n",
      "894 710 773 706 503 826 790 735\n",
      "epoch= 2\n",
      "val_loss= 0.3909729979932308\n",
      "________________________________\n",
      "946 839 868 841 722 949 850 820\n",
      "epoch= 2\n",
      "val_loss= 0.31972868740558624\n",
      "________________________________\n",
      "977 902 827 856 831 828 909 881\n",
      "epoch= 2\n",
      "val_loss= 0.5070053040981293\n",
      "________________________________\n",
      "907 738 787 744 864 860 509 791\n",
      "epoch= 2\n",
      "test_loss= 0.5479478810978408\n",
      "________________________________\n",
      "886 790 753 767 736 850 715 729\n",
      "3873\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [43:29<00:00,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 3\n",
      "train_running_loss= 0.47885554166644695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 3\n",
      "val_loss= 0.5665826341685127\n",
      "________________________________\n",
      "860 706 757 701 529 770 803 728\n",
      "epoch= 3\n",
      "val_loss= 0.5046874989162792\n",
      "________________________________\n",
      "906 723 798 725 554 813 809 757\n",
      "epoch= 3\n",
      "val_loss= 0.35112149609873694\n",
      "________________________________\n",
      "952 842 888 852 742 954 860 833\n",
      "epoch= 3\n",
      "val_loss= 0.32569846160271587\n",
      "________________________________\n",
      "966 885 857 867 831 860 911 887\n",
      "epoch= 3\n",
      "val_loss= 0.4261474473909898\n",
      "________________________________\n",
      "916 769 801 779 908 854 575 846\n",
      "epoch= 3\n",
      "test_loss= 0.5130009845931931\n",
      "________________________________\n",
      "887 794 771 782 785 834 727 760\n",
      "3924\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [44:07<00:00,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4\n",
      "train_running_loss= 0.4786740951531431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4\n",
      "val_loss= 0.45358895203646493\n",
      "________________________________\n",
      "862 702 697 700 469 756 874 800\n",
      "epoch= 4\n",
      "val_loss= 0.42413666302507574\n",
      "________________________________\n",
      "912 747 763 754 521 867 873 823\n",
      "epoch= 4\n",
      "val_loss= 0.3463541101664305\n",
      "________________________________\n",
      "941 836 833 834 671 954 877 836\n",
      "epoch= 4\n",
      "val_loss= 0.31026844242039847\n",
      "________________________________\n",
      "974 886 790 827 691 890 899 867\n",
      "epoch= 4\n",
      "val_loss= 0.5782341234611742\n",
      "________________________________\n",
      "898 729 777 722 828 869 469 747\n",
      "epoch= 4\n",
      "test_loss= 0.6680831929834763\n",
      "________________________________\n",
      "881 738 702 656 429 884 655 579\n",
      "3837\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:56<00:00,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 5\n",
      "train_running_loss= 0.4770851014412516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 5\n",
      "val_loss= 0.45301807452650633\n",
      "________________________________\n",
      "882 725 766 739 581 767 870 805\n",
      "epoch= 5\n",
      "val_loss= 0.4086294716054743\n",
      "________________________________\n",
      "922 749 789 763 546 877 867 820\n",
      "epoch= 5\n",
      "val_loss= 0.31903640863796073\n",
      "________________________________\n",
      "950 847 877 858 730 969 876 844\n",
      "epoch= 5\n",
      "val_loss= 0.269680116983021\n",
      "________________________________\n",
      "975 916 870 891 843 900 930 910\n",
      "epoch= 5\n",
      "val_loss= 0.4368005087881377\n",
      "________________________________\n",
      "920 768 808 778 896 876 560 832\n",
      "epoch= 5\n",
      "test_loss= 0.5876501301432601\n",
      "________________________________\n",
      "901 776 773 748 622 922 701 673\n",
      "4029\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [43:52<00:00,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6\n",
      "train_running_loss= 0.4602946079474017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6\n",
      "val_loss= 0.5490322375998777\n",
      "________________________________\n",
      "877 708 784 723 565 772 833 763\n",
      "epoch= 6\n",
      "val_loss= 0.511596755547957\n",
      "________________________________\n",
      "914 725 827 744 541 871 819 773\n",
      "epoch= 6\n",
      "val_loss= 0.3507813944791754\n",
      "________________________________\n",
      "951 846 892 858 740 970 865 838\n",
      "epoch= 6\n",
      "val_loss= 0.25768125583143797\n",
      "________________________________\n",
      "976 898 888 892 843 908 926 908\n",
      "epoch= 6\n",
      "val_loss= 0.32501240300409723\n",
      "________________________________\n",
      "931 815 822 818 934 898 623 886\n",
      "epoch= 6\n",
      "test_loss= 0.5784825141182041\n",
      "________________________________\n",
      "883 756 776 755 682 881 702 697\n",
      "4035\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [42:19<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 7\n",
      "train_running_loss= 0.43852402606596147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 7\n",
      "val_loss= 0.6044826735468471\n",
      "________________________________\n",
      "858 676 768 691 526 743 804 727\n",
      "epoch= 7\n",
      "val_loss= 0.5119881955060092\n",
      "________________________________\n",
      "911 712 802 727 504 870 807 757\n",
      "epoch= 7\n",
      "val_loss= 0.342882907949388\n",
      "________________________________\n",
      "951 842 886 853 733 964 861 833\n",
      "epoch= 7\n",
      "val_loss= 0.24950866576503306\n",
      "________________________________\n",
      "977 904 893 897 854 907 930 912\n",
      "epoch= 7\n",
      "val_loss= 0.36595398458567535\n",
      "________________________________\n",
      "921 794 813 803 925 877 606 872\n",
      "epoch= 7\n",
      "test_loss= 0.5622150399307213\n",
      "________________________________\n",
      "885 771 754 744 639 900 693 675\n",
      "3971\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:28<00:00,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 8\n",
      "train_running_loss= 0.40661338689363885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 8\n",
      "val_loss= 0.46240048198138967\n",
      "________________________________\n",
      "882 719 753 732 573 751 873 807\n",
      "epoch= 8\n",
      "val_loss= 0.456866673447869\n",
      "________________________________\n",
      "919 736 816 761 570 861 851 805\n",
      "epoch= 8\n",
      "val_loss= 0.3182666419694821\n",
      "________________________________\n",
      "950 853 881 864 746 961 884 854\n",
      "epoch= 8\n",
      "val_loss= 0.2390103156075758\n",
      "________________________________\n",
      "979 915 891 902 871 901 934 917\n",
      "epoch= 8\n",
      "val_loss= 0.37436017394065857\n",
      "________________________________\n",
      "921 777 813 791 910 885 577 852\n",
      "epoch= 8\n",
      "test_loss= 0.48631764992628945\n",
      "________________________________\n",
      "892 808 806 807 797 895 728 770\n",
      "4050\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:41<00:00,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 9\n",
      "train_running_loss= 0.4365870452632003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 9\n",
      "val_loss= 0.52555772486855\n",
      "________________________________\n",
      "877 700 779 725 575 747 852 783\n",
      "epoch= 9\n",
      "val_loss= 0.4681144519285722\n",
      "________________________________\n",
      "926 735 815 757 554 872 845 799\n",
      "epoch= 9\n",
      "val_loss= 0.32680399684856337\n",
      "________________________________\n",
      "956 852 897 867 752 970 878 851\n",
      "epoch= 9\n",
      "val_loss= 0.22916384479578802\n",
      "________________________________\n",
      "980 909 894 901 850 919 933 916\n",
      "epoch= 9\n",
      "val_loss= 0.3279544376965725\n",
      "________________________________\n",
      "927 798 825 810 922 908 602 871\n",
      "epoch= 9\n",
      "test_loss= 0.543131872421444\n",
      "________________________________\n",
      "882 757 773 751 654 911 689 679\n",
      "4060\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:13<00:00,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10\n",
      "train_running_loss= 0.4470686864295563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 10\n",
      "val_loss= 0.4403530832599191\n",
      "________________________________\n",
      "880 731 774 748 585 783 877 814\n",
      "epoch= 10\n",
      "val_loss= 0.41873677752234717\n",
      "________________________________\n",
      "925 734 786 754 532 870 860 811\n",
      "epoch= 10\n",
      "val_loss= 0.2832660907879472\n",
      "________________________________\n",
      "960 864 891 875 763 968 895 867\n",
      "epoch= 10\n",
      "val_loss= 0.2532529296243892\n",
      "________________________________\n",
      "978 922 879 899 847 913 935 916\n",
      "epoch= 10\n",
      "val_loss= 0.41188446229154413\n",
      "________________________________\n",
      "927 777 813 785 896 893 566 834\n",
      "epoch= 10\n",
      "test_loss= 0.6032737421222253\n",
      "________________________________\n",
      "895 751 754 713 553 907 681 636\n",
      "4061\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:12<00:00,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 11\n",
      "train_running_loss= 0.4090898107437613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 11\n",
      "val_loss= 0.45685697303098793\n",
      "________________________________\n",
      "875 735 759 743 583 768 877 813\n",
      "epoch= 11\n",
      "val_loss= 0.43183871561830695\n",
      "________________________________\n",
      "919 736 791 756 545 862 860 810\n",
      "epoch= 11\n",
      "val_loss= 0.28966862459977466\n",
      "________________________________\n",
      "959 866 880 872 751 970 895 865\n",
      "epoch= 11\n",
      "val_loss= 0.26563361374770894\n",
      "________________________________\n",
      "977 923 874 896 858 897 932 913\n",
      "epoch= 11\n",
      "val_loss= 0.3736308641505964\n",
      "________________________________\n",
      "929 784 824 799 908 900 587 852\n",
      "epoch= 11\n",
      "test_loss= 0.5304129699079116\n",
      "________________________________\n",
      "889 784 787 781 725 900 716 726\n",
      "4066\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:08<00:00,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 12\n",
      "train_running_loss= 0.4406619881861055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 12\n",
      "val_loss= 0.49211471746949587\n",
      "________________________________\n",
      "886 739 788 746 593 789 855 790\n",
      "epoch= 12\n",
      "val_loss= 0.47208748080513696\n",
      "________________________________\n",
      "927 744 831 765 582 868 845 800\n",
      "epoch= 12\n",
      "val_loss= 0.3280708060289423\n",
      "________________________________\n",
      "954 848 897 860 743 972 865 838\n",
      "epoch= 12\n",
      "val_loss= 0.2377710289814893\n",
      "________________________________\n",
      "979 913 899 905 868 910 936 919\n",
      "epoch= 12\n",
      "val_loss= 0.34772359602379077\n",
      "________________________________\n",
      "922 807 819 812 922 907 607 871\n",
      "epoch= 12\n",
      "test_loss= 0.490843865393412\n",
      "________________________________\n",
      "884 824 815 816 811 916 723 779\n",
      "4088\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:19<00:00,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 13\n",
      "train_running_loss= 0.4490661210090376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 13\n",
      "val_loss= 0.46852702428312865\n",
      "________________________________\n",
      "888 732 768 742 588 768 869 805\n",
      "epoch= 13\n",
      "val_loss= 0.46019698543982074\n",
      "________________________________\n",
      "921 747 821 765 571 874 849 804\n",
      "epoch= 13\n",
      "val_loss= 0.3055512147645156\n",
      "________________________________\n",
      "956 863 895 874 760 974 889 861\n",
      "epoch= 13\n",
      "val_loss= 0.2552458889344159\n",
      "________________________________\n",
      "979 920 875 895 857 896 932 912\n",
      "epoch= 13\n",
      "val_loss= 0.3662477492383032\n",
      "________________________________\n",
      "913 794 811 802 918 900 586 865\n",
      "epoch= 13\n",
      "test_loss= 0.5232153160147147\n",
      "________________________________\n",
      "895 807 804 804 767 909 735 756\n",
      "4078\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [41:05<00:00,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 14\n",
      "train_running_loss= 0.42198206473115923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 14\n",
      "val_loss= 0.5345723260851467\n",
      "________________________________\n",
      "867 708 773 718 571 749 835 765\n",
      "epoch= 14\n",
      "val_loss= 0.466544126922434\n",
      "________________________________\n",
      "917 742 824 755 569 862 835 788\n",
      "epoch= 14\n",
      "val_loss= 0.31786610341320437\n",
      "________________________________\n",
      "953 859 901 869 760 970 875 850\n",
      "epoch= 14\n",
      "val_loss= 0.2562720381161746\n",
      "________________________________\n",
      "978 905 884 890 863 881 927 907\n",
      "epoch= 14\n",
      "val_loss= 0.3473144534862403\n",
      "________________________________\n",
      "931 810 817 812 930 878 629 878\n",
      "epoch= 14\n",
      "test_loss= 0.49408909634198295\n",
      "________________________________\n",
      "875 827 795 805 818 884 713 779\n",
      "4044\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:55<00:00,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 15\n",
      "train_running_loss= 0.38266571921336684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 15\n",
      "val_loss= 0.4503937083132127\n",
      "________________________________\n",
      "880 745 762 750 591 777 882 820\n",
      "epoch= 15\n",
      "val_loss= 0.4451004483483054\n",
      "________________________________\n",
      "927 743 805 761 559 871 854 807\n",
      "epoch= 15\n",
      "val_loss= 0.26921708943943184\n",
      "________________________________\n",
      "964 877 898 886 777 976 905 878\n",
      "epoch= 15\n",
      "val_loss= 0.2506093409131555\n",
      "________________________________\n",
      "976 922 882 900 862 904 934 916\n",
      "epoch= 15\n",
      "val_loss= 0.471388887275349\n",
      "________________________________\n",
      "918 763 820 772 872 906 539 806\n",
      "epoch= 15\n",
      "test_loss= 0.5051977529679195\n",
      "________________________________\n",
      "894 798 803 799 763 903 731 752\n",
      "4069\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:54<00:00,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 16\n",
      "train_running_loss= 0.453484281369283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 16\n",
      "val_loss= 0.5158669597962323\n",
      "________________________________\n",
      "884 737 781 738 587 779 850 783\n",
      "epoch= 16\n",
      "val_loss= 0.49158271063457837\n",
      "________________________________\n",
      "921 729 807 742 544 857 824 775\n",
      "epoch= 16\n",
      "val_loss= 0.32190191683669883\n",
      "________________________________\n",
      "958 861 894 868 761 964 879 852\n",
      "epoch= 16\n",
      "val_loss= 0.23740425092332504\n",
      "________________________________\n",
      "981 908 892 898 862 899 931 913\n",
      "epoch= 16\n",
      "val_loss= 0.3197290500005086\n",
      "________________________________\n",
      "932 827 802 814 931 883 627 882\n",
      "epoch= 16\n",
      "test_loss= 0.4779377845254275\n",
      "________________________________\n",
      "901 820 814 817 795 908 747 777\n",
      "4060\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:51<00:00,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 17\n",
      "train_running_loss= 0.4018403609912658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 17\n",
      "val_loss= 0.4176929014570573\n",
      "________________________________\n",
      "891 764 775 767 615 793 892 834\n",
      "epoch= 17\n",
      "val_loss= 0.41525276140733197\n",
      "________________________________\n",
      "927 755 795 771 569 869 874 828\n",
      "epoch= 17\n",
      "val_loss= 0.27995449180404347\n",
      "________________________________\n",
      "960 864 883 873 751 972 895 865\n",
      "epoch= 17\n",
      "val_loss= 0.2639308405273101\n",
      "________________________________\n",
      "977 926 873 896 860 896 933 914\n",
      "epoch= 17\n",
      "val_loss= 0.4811577480850798\n",
      "________________________________\n",
      "914 755 811 766 869 902 526 802\n",
      "epoch= 17\n",
      "test_loss= 0.5462225290513275\n",
      "________________________________\n",
      "894 782 781 769 684 910 712 705\n",
      "4073\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:45<00:00,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 18\n",
      "train_running_loss= 0.43563064364417003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 18\n",
      "val_loss= 0.46486902412246256\n",
      "________________________________\n",
      "879 742 745 741 578 763 883 820\n",
      "epoch= 18\n",
      "val_loss= 0.4443213153969158\n",
      "________________________________\n",
      "922 745 791 762 556 865 866 818\n",
      "epoch= 18\n",
      "val_loss= 0.3089850206548969\n",
      "________________________________\n",
      "955 862 870 864 739 965 889 857\n",
      "epoch= 18\n",
      "val_loss= 0.2745852663236506\n",
      "________________________________\n",
      "971 915 864 887 816 916 929 908\n",
      "epoch= 18\n",
      "val_loss= 0.4491723116600152\n",
      "________________________________\n",
      "911 774 793 771 892 874 548 825\n",
      "epoch= 18\n",
      "test_loss= 0.576234490564554\n",
      "________________________________\n",
      "888 767 792 766 678 912 707 700\n",
      "4025\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:39<00:00,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 19\n",
      "train_running_loss= 0.392137099746971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 19\n",
      "val_loss= 0.49398131931529327\n",
      "________________________________\n",
      "881 734 773 741 575 789 859 792\n",
      "epoch= 19\n",
      "val_loss= 0.4586122984235937\n",
      "________________________________\n",
      "928 744 818 763 566 873 849 803\n",
      "epoch= 19\n",
      "val_loss= 0.2857541187355916\n",
      "________________________________\n",
      "964 873 910 885 781 979 895 871\n",
      "epoch= 19\n",
      "val_loss= 0.22324220178758397\n",
      "________________________________\n",
      "981 923 900 911 877 913 942 925\n",
      "epoch= 19\n",
      "val_loss= 0.319331885287256\n",
      "________________________________\n",
      "931 823 828 825 932 907 637 886\n",
      "epoch= 19\n",
      "test_loss= 0.4838741296293712\n",
      "________________________________\n",
      "902 809 833 820 785 926 749 773\n",
      "4125\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:50<00:00,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 20\n",
      "train_running_loss= 0.36449351575020356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 20\n",
      "val_loss= 0.4483456225956188\n",
      "________________________________\n",
      "884 732 744 736 551 780 878 811\n",
      "epoch= 20\n",
      "val_loss= 0.4172517115419561\n",
      "________________________________\n",
      "927 762 792 776 574 871 882 837\n",
      "epoch= 20\n",
      "val_loss= 0.29131948451201123\n",
      "________________________________\n",
      "957 869 870 869 732 979 897 865\n",
      "epoch= 20\n",
      "val_loss= 0.24270125347025254\n",
      "________________________________\n",
      "981 938 886 909 878 909 940 924\n",
      "epoch= 20\n",
      "val_loss= 0.4370900428656376\n",
      "________________________________\n",
      "926 766 828 785 891 900 564 829\n",
      "epoch= 20\n",
      "test_loss= 0.685889796602844\n",
      "________________________________\n",
      "904 767 726 682 454 924 668 597\n",
      "4075\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:55<00:00,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 21\n",
      "train_running_loss= 0.388298922628297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 21\n",
      "val_loss= 0.433178409057505\n",
      "________________________________\n",
      "882 744 751 747 577 775 888 826\n",
      "epoch= 21\n",
      "val_loss= 0.4092595495960929\n",
      "________________________________\n",
      "923 752 776 763 544 867 878 831\n",
      "epoch= 21\n",
      "val_loss= 0.273569175042212\n",
      "________________________________\n",
      "963 877 885 881 760 978 905 876\n",
      "epoch= 21\n",
      "val_loss= 0.2497283325475805\n",
      "________________________________\n",
      "980 932 863 892 834 911 933 913\n",
      "epoch= 21\n",
      "val_loss= 0.43737563129627344\n",
      "________________________________\n",
      "914 761 820 778 882 907 545 819\n",
      "epoch= 21\n",
      "test_loss= 0.6125249356621563\n",
      "________________________________\n",
      "895 754 749 714 557 904 681 637\n",
      "4061\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [40:44<00:00,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 22\n",
      "train_running_loss= 0.41608257910027274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 22\n",
      "val_loss= 0.4931129953440498\n",
      "________________________________\n",
      "880 730 760 738 582 761 871 806\n",
      "epoch= 22\n",
      "val_loss= 0.4349214786832983\n",
      "________________________________\n",
      "927 747 810 768 584 857 862 815\n",
      "epoch= 22\n",
      "val_loss= 0.2886514014874895\n",
      "________________________________\n",
      "963 869 899 880 774 971 896 869\n",
      "epoch= 22\n",
      "val_loss= 0.24696071708903594\n",
      "________________________________\n",
      "979 916 881 896 867 890 932 913\n",
      "epoch= 22\n",
      "val_loss= 0.36150579380266595\n",
      "________________________________\n",
      "929 791 808 795 917 864 602 859\n",
      "epoch= 22\n",
      "test_loss= 0.5684818772986384\n",
      "________________________________\n",
      "885 774 763 756 675 893 702 694\n",
      "4077\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [44:47<00:00,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 23\n",
      "train_running_loss= 0.4076438024048863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 23\n",
      "val_loss= 0.47602589165463166\n",
      "________________________________\n",
      "880 722 764 737 578 762 870 805\n",
      "epoch= 23\n",
      "val_loss= 0.44323966719887475\n",
      "________________________________\n",
      "926 755 822 776 592 872 865 821\n",
      "epoch= 23\n",
      "val_loss= 0.2884273948147893\n",
      "________________________________\n",
      "960 869 895 879 768 974 897 869\n",
      "epoch= 23\n",
      "val_loss= 0.23504630607717178\n",
      "________________________________\n",
      "979 920 890 904 858 916 937 919\n",
      "epoch= 23\n",
      "val_loss= 0.34300896660848096\n",
      "________________________________\n",
      "928 798 828 811 923 894 617 872\n",
      "epoch= 23\n",
      "test_loss= 0.5053161272022983\n",
      "________________________________\n",
      "892 803 797 799 771 892 733 757\n",
      "4107\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [44:23<00:00,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 24\n",
      "train_running_loss= 0.40869146532308626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 24\n",
      "val_loss= 0.44701239116051616\n",
      "________________________________\n",
      "877 747 762 751 583 789 883 820\n",
      "epoch= 24\n",
      "val_loss= 0.43184754794294183\n",
      "________________________________\n",
      "925 750 809 770 571 875 865 820\n",
      "epoch= 24\n",
      "val_loss= 0.29685323778539896\n",
      "________________________________\n",
      "954 863 881 871 747 972 895 864\n",
      "epoch= 24\n",
      "val_loss= 0.2602373002206578\n",
      "________________________________\n",
      "977 920 880 898 856 905 934 915\n",
      "epoch= 24\n",
      "val_loss= 0.4173912026665427\n",
      "________________________________\n",
      "898 770 801 781 899 897 549 839\n",
      "epoch= 24\n",
      "test_loss= 0.5124952778367713\n",
      "________________________________\n",
      "898 792 815 800 759 902 739 753\n",
      "4071\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [47:23<00:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 25\n",
      "train_running_loss= 0.3753766917184056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 25\n",
      "val_loss= 0.5583432719987982\n",
      "________________________________\n",
      "879 727 751 727 580 741 860 793\n",
      "epoch= 25\n",
      "val_loss= 0.5576579001816836\n",
      "________________________________\n",
      "917 741 814 751 574 844 836 787\n",
      "epoch= 25\n",
      "val_loss= 0.33170214233299095\n",
      "________________________________\n",
      "960 869 902 880 777 969 893 868\n",
      "epoch= 25\n",
      "val_loss= 0.24867371162947485\n",
      "________________________________\n",
      "980 920 885 900 882 884 933 915\n",
      "epoch= 25\n",
      "val_loss= 0.3634392826846152\n",
      "________________________________\n",
      "928 788 811 795 914 877 593 856\n",
      "epoch= 25\n",
      "test_loss= 0.5199837486932773\n",
      "________________________________\n",
      "883 815 791 801 806 877 720 773\n",
      "4053\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [49:50<00:00,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 26\n",
      "train_running_loss= 0.37408913615268635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 26\n",
      "val_loss= 0.46904008002842174\n",
      "________________________________\n",
      "891 731 785 745 600 771 865 801\n",
      "epoch= 26\n",
      "val_loss= 0.4628695303743536\n",
      "________________________________\n",
      "927 740 822 760 571 866 844 798\n",
      "epoch= 26\n",
      "val_loss= 0.2918475028127432\n",
      "________________________________\n",
      "962 867 904 878 767 979 888 862\n",
      "epoch= 26\n",
      "val_loss= 0.23077605752383962\n",
      "________________________________\n",
      "980 916 899 907 867 916 937 921\n",
      "epoch= 26\n",
      "val_loss= 0.3413746113126928\n",
      "________________________________\n",
      "932 806 827 815 923 900 622 873\n",
      "epoch= 26\n",
      "test_loss= 0.5949000548018087\n",
      "________________________________\n",
      "886 783 794 781 715 914 715 721\n",
      "4105\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [48:23<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 27\n",
      "train_running_loss= 0.4298764592072422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 27\n",
      "val_loss= 0.5029363316648147\n",
      "________________________________\n",
      "889 737 784 749 603 775 870 807\n",
      "epoch= 27\n",
      "val_loss= 0.4409770667552948\n",
      "________________________________\n",
      "928 745 807 765 575 857 862 814\n",
      "epoch= 27\n",
      "val_loss= 0.3140879866356651\n",
      "________________________________\n",
      "961 865 902 877 768 974 890 863\n",
      "epoch= 27\n",
      "val_loss= 0.2479015176787096\n",
      "________________________________\n",
      "981 920 890 903 878 897 935 918\n",
      "epoch= 27\n",
      "val_loss= 0.36525249300581036\n",
      "________________________________\n",
      "920 774 818 793 911 889 579 854\n",
      "epoch= 27\n",
      "test_loss= 0.5777065609351243\n",
      "________________________________\n",
      "893 775 777 760 671 900 710 697\n",
      "4087\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [48:03<00:00,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 28\n",
      "train_running_loss= 0.36311035399136754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 28\n",
      "val_loss= 0.4316262637867647\n",
      "________________________________\n",
      "885 738 766 749 592 771 883 821\n",
      "epoch= 28\n",
      "val_loss= 0.4147391806949269\n",
      "________________________________\n",
      "927 756 807 774 586 865 871 826\n",
      "epoch= 28\n",
      "val_loss= 0.27691813341031474\n",
      "________________________________\n",
      "962 875 892 882 770 973 902 875\n",
      "epoch= 28\n",
      "val_loss= 0.2385078414398081\n",
      "________________________________\n",
      "980 929 887 906 867 913 939 922\n",
      "epoch= 28\n",
      "val_loss= 0.36956593213659344\n",
      "________________________________\n",
      "921 793 836 811 914 910 608 862\n",
      "epoch= 28\n",
      "test_loss= 0.5404226360934796\n",
      "________________________________\n",
      "904 798 799 789 727 907 735 736\n",
      "4122\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [52:59<00:00,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 29\n",
      "train_running_loss= 0.4029159977600343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 29\n",
      "val_loss= 0.4783608071944293\n",
      "________________________________\n",
      "882 753 745 742 576 776 874 809\n",
      "epoch= 29\n",
      "val_loss= 0.47687105699019\n",
      "________________________________\n",
      "923 747 795 760 563 860 859 810\n",
      "epoch= 29\n",
      "val_loss= 0.28780442103743553\n",
      "________________________________\n",
      "963 880 888 883 774 971 904 877\n",
      "epoch= 29\n",
      "val_loss= 0.25464343849350424\n",
      "________________________________\n",
      "980 919 892 903 877 895 937 918\n",
      "epoch= 29\n",
      "val_loss= 0.4247997493454904\n",
      "________________________________\n",
      "924 778 803 779 892 889 555 828\n",
      "epoch= 29\n",
      "test_loss= 0.5283454542998041\n",
      "________________________________\n",
      "881 799 776 787 756 894 710 739\n",
      "4067\n",
      "max_epoch= 19\n",
      "max_val_f1_all= 4125\n",
      "4125 19\n",
      "_____________H_____________::\n",
      "['902', '809', '833', '820', '785', '926', '749', '773']\n",
      "___________________________::\n"
     ]
    }
   ],
   "source": [
    "# 更改加载的教师模型字典：\n",
    "# 加载5项不同的教师模型\n",
    "# 开始训练流程\n",
    "Teacher_queue=TeacherQueue(max_size=CRL_queue_len)\n",
    "student_model=generate_model(base_model=base_model_student,input_channels=2, num_classes=class_num,DG_method=DG_method_student,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model=generate_model(base_model=base_model_teacher,input_channels=12, num_classes=class_num,DG_method=DG_method_teacher,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model.load_state_dict(torch.load('/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/1711094236_ABCLN_H_proposed_cnnAg_zong_f1.pth'))\n",
    "teacher_model.eval()\n",
    "print('ok')\n",
    "optimizer=generate_optimizer(base_optimizer=base_optimizer,model_parameters=student_model.parameters(),SAM_flag=False,weight_decay=1e-4)\n",
    "\n",
    "mdst_path='/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/'\n",
    "mdst_name=str(int(time.time()))+'_ABCLN_H_zishiying_proposed_xuesheng_cnnAg_zong_f1.pth'\n",
    "\n",
    "max_val_f1_all=0\n",
    "max_epoch=0\n",
    "max_test_H_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "# max_test_R_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    print('_____________________________________')\n",
    "    print(' ')\n",
    "    time.sleep(0.1)\n",
    "    train_student_one_epoch(epoch,warm_epoch,total_epoch,student_model,teacher_model, \\\n",
    "                            max_lr,min_lr,loss_fn_clf,loss_fn_tslb,optimizer, \\\n",
    "                            feature_contrastive_loss_fn, \\\n",
    "                            Teacher_queue,base_optimizer, \\\n",
    "                            train_datasloader,device,GRL_loss_beta,GAN_aug='0211',GAN_model=cls_model)\n",
    "    \n",
    "    auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_A, device)\n",
    "    print(auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_B, device)\n",
    "    print(auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_C, device)\n",
    "    print(auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_N, device)\n",
    "    print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_L, device)\n",
    "    print(auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc)\n",
    "#     auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           val_datasloader_N, device)\n",
    "#     print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "                                         \n",
    "    H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          test_H_datasloader, device)\n",
    "    print(H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc)\n",
    "\n",
    "#     R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           test_R_datasloader, device)\n",
    "#     print(R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc)\n",
    "    \n",
    "    f1_val_all=int(f1_val_A)+int(f1_val_B)+int(f1_val_C)+int(f1_val_N)+int(f1_val_L)\n",
    "    \n",
    "    if int(f1_val_all)>max_val_f1_all:\n",
    "        max_val_f1_all=int(f1_val_all)\n",
    "        max_epoch=epoch\n",
    "        max_model_dict=student_model.state_dict()\n",
    "        # 指定你想要保存模型参数的路径  \n",
    "        torch.save(max_model_dict, os.path.join(mdst_path, mdst_name))\n",
    "        # 使用torch.save()保存模型参数  \n",
    "        \n",
    "        max_test_H_dict['auc']=H_auc\n",
    "        max_test_H_dict['pre']=H_pre\n",
    "        max_test_H_dict['rec']=H_rec\n",
    "        max_test_H_dict['f1_test']=H_f1_test\n",
    "        max_test_H_dict['f1_test_normal']=H_f1_test_normal\n",
    "        max_test_H_dict['f1_test_AF']=H_f1_test_AF\n",
    "        max_test_H_dict['f1_test_Other']=H_f1_test_Other\n",
    "        max_test_H_dict['acc']=H_acc\n",
    "        \n",
    "    print(f1_val_all)\n",
    "        \n",
    "print('max_epoch=',max_epoch)\n",
    "print('max_val_f1_all=',max_val_f1_all)\n",
    "        \n",
    "print (max_val_f1_all,max_epoch)\n",
    "print('_____________H_____________::')\n",
    "print(list(max_test_H_dict.values()))\n",
    "print('___________________________::')                                         \n",
    "\n",
    "\n",
    "\n",
    "# 写入csv\n",
    "\n",
    "header=[\n",
    "        'train\\\\test\\t',\n",
    "        'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "#         'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "\n",
    "        # round\n",
    "\n",
    "results=[\n",
    "        [\n",
    "        'H',\n",
    "        max_test_H_dict['auc'],\n",
    "        max_test_H_dict['pre'],\n",
    "        max_test_H_dict['rec'],\n",
    "        max_test_H_dict['f1_test'],\n",
    "        max_test_H_dict['f1_test_normal'],\n",
    "        max_test_H_dict['f1_test_AF'],\n",
    "        max_test_H_dict['f1_test_Other'],\n",
    "        max_test_H_dict['acc']+'\\t',\n",
    "         ],\n",
    "        ]\n",
    "\n",
    "with open('log/proposed_ABCLN_H_zishiying_600000_20240321_total_f1.csv','a',encoding='utf-8')as file_obj:\n",
    "    writer=csv.writer(file_obj)\n",
    "    writer.writerow(header)\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c3449a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model=cnn_Ag\n",
      "base_model=cnn_Ag_teacher_self_distill_leadI\n",
      "ok\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [51:27<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0\n",
      "train_running_loss= 0.5448775227047273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0\n",
      "val_loss= 0.48357677985640135\n",
      "________________________________\n",
      "859 678 694 682 526 663 857 783\n",
      "epoch= 0\n",
      "val_loss= 0.5214140090075406\n",
      "________________________________\n",
      "897 708 777 722 521 827 820 766\n",
      "epoch= 0\n",
      "val_loss= 0.36727265765269596\n",
      "________________________________\n",
      "938 832 828 828 703 916 867 828\n",
      "epoch= 0\n",
      "val_loss= 0.37340833860285144\n",
      "________________________________\n",
      "968 889 807 841 786 833 902 871\n",
      "epoch= 0\n",
      "val_loss= 0.9342816399805474\n",
      "________________________________\n",
      "899 717 751 667 737 837 427 652\n",
      "epoch= 0\n",
      "test_loss= 0.6295038873311316\n",
      "________________________________\n",
      "868 757 698 716 681 780 687 686\n",
      "3740\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [47:34<00:00,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1\n",
      "train_running_loss= 0.557854641568603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1\n",
      "val_loss= 0.5573014669558581\n",
      "________________________________\n",
      "863 678 781 696 553 725 808 736\n",
      "epoch= 1\n",
      "val_loss= 0.5809561041268435\n",
      "________________________________\n",
      "892 693 791 695 499 825 761 713\n",
      "epoch= 1\n",
      "val_loss= 0.4624468150238196\n",
      "________________________________\n",
      "928 803 862 806 686 932 800 774\n",
      "epoch= 1\n",
      "val_loss= 0.29088525035802054\n",
      "________________________________\n",
      "970 896 869 881 849 874 921 900\n",
      "epoch= 1\n",
      "val_loss= 0.4635782919146798\n",
      "________________________________\n",
      "884 730 762 742 887 848 490 816\n",
      "epoch= 1\n",
      "test_loss= 0.5525074589370501\n",
      "________________________________\n",
      "881 754 773 760 738 828 714 729\n",
      "3820\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [49:45<00:00,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2\n",
      "train_running_loss= 0.5009948250383292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2\n",
      "val_loss= 0.47048768050530376\n",
      "________________________________\n",
      "884 750 752 732 614 717 865 801\n",
      "epoch= 2\n",
      "val_loss= 0.5168036439202048\n",
      "________________________________\n",
      "906 739 776 727 547 813 821 767\n",
      "epoch= 2\n",
      "val_loss= 0.36344506156941253\n",
      "________________________________\n",
      "950 842 856 837 745 907 860 830\n",
      "epoch= 2\n",
      "val_loss= 0.40058181040427265\n",
      "________________________________\n",
      "964 909 800 839 842 772 903 870\n",
      "epoch= 2\n",
      "val_loss= 0.6152540228583596\n",
      "________________________________\n",
      "904 722 749 704 835 809 468 747\n",
      "epoch= 2\n",
      "test_loss= 0.5532562478934184\n",
      "________________________________\n",
      "899 791 712 736 709 779 720 716\n",
      "3839\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [47:43<00:00,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 3\n",
      "train_running_loss= 0.5071567573291444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 3\n",
      "val_loss= 0.4699901149553411\n",
      "________________________________\n",
      "878 732 784 742 581 791 853 787\n",
      "epoch= 3\n",
      "val_loss= 0.473095872185447\n",
      "________________________________\n",
      "908 722 803 741 545 848 829 780\n",
      "epoch= 3\n",
      "val_loss= 0.33575219195336103\n",
      "________________________________\n",
      "949 848 877 855 741 952 870 840\n",
      "epoch= 3\n",
      "val_loss= 0.2990909420392093\n",
      "________________________________\n",
      "972 905 867 884 843 884 924 902\n",
      "epoch= 3\n",
      "val_loss= 0.5368923833875945\n",
      "________________________________\n",
      "881 727 750 724 858 851 463 779\n",
      "epoch= 3\n",
      "test_loss= 0.5396371076602747\n",
      "________________________________\n",
      "891 753 809 775 746 852 725 739\n",
      "3946\n",
      "_____________________________________\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|██████████████████████████████████████████████▌                                               | 245/495 [23:10<25:50,  6.20s/it]"
     ]
    }
   ],
   "source": [
    "# 更改加载的教师模型字典：\n",
    "# 加载5项不同的教师模型\n",
    "# 开始训练流程\n",
    "Teacher_queue=TeacherQueue(max_size=CRL_queue_len)\n",
    "student_model=generate_model(base_model=base_model_student,input_channels=2, num_classes=class_num,DG_method=DG_method_student,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model=generate_model(base_model=base_model_teacher,input_channels=12, num_classes=class_num,DG_method=DG_method_teacher,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model.load_state_dict(torch.load('/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/1711092851_ABCLN_H_proposed_cnnAg_zong_f1.pth'))\n",
    "teacher_model.eval()\n",
    "print('ok')\n",
    "optimizer=generate_optimizer(base_optimizer=base_optimizer,model_parameters=student_model.parameters(),SAM_flag=False,weight_decay=1e-4)\n",
    "\n",
    "mdst_path='/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/'\n",
    "mdst_name=str(int(time.time()))+'_ABCLN_H_zishiying_proposed_xuesheng_cnnAg_zong_f1.pth'\n",
    "\n",
    "max_val_f1_all=0\n",
    "max_epoch=0\n",
    "max_test_H_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "# max_test_R_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    print('_____________________________________')\n",
    "    print(' ')\n",
    "    time.sleep(0.1)\n",
    "    train_student_one_epoch(epoch,warm_epoch,total_epoch,student_model,teacher_model, \\\n",
    "                            max_lr,min_lr,loss_fn_clf,loss_fn_tslb,optimizer, \\\n",
    "                            feature_contrastive_loss_fn, \\\n",
    "                            Teacher_queue,base_optimizer, \\\n",
    "                            train_datasloader,device,GRL_loss_beta,GAN_aug='0211',GAN_model=cls_model)\n",
    "    \n",
    "    auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_A, device)\n",
    "    print(auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_B, device)\n",
    "    print(auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_C, device)\n",
    "    print(auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_N, device)\n",
    "    print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_L, device)\n",
    "    print(auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc)\n",
    "#     auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           val_datasloader_N, device)\n",
    "#     print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "                                         \n",
    "    H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          test_H_datasloader, device)\n",
    "    print(H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc)\n",
    "\n",
    "#     R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           test_R_datasloader, device)\n",
    "#     print(R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc)\n",
    "    \n",
    "    f1_val_all=int(f1_val_A)+int(f1_val_B)+int(f1_val_C)+int(f1_val_N)+int(f1_val_L)\n",
    "    \n",
    "    if int(f1_val_all)>max_val_f1_all:\n",
    "        max_val_f1_all=int(f1_val_all)\n",
    "        max_epoch=epoch\n",
    "        max_model_dict=student_model.state_dict()\n",
    "        # 指定你想要保存模型参数的路径  \n",
    "        torch.save(max_model_dict, os.path.join(mdst_path, mdst_name))\n",
    "        # 使用torch.save()保存模型参数  \n",
    "        \n",
    "        max_test_H_dict['auc']=H_auc\n",
    "        max_test_H_dict['pre']=H_pre\n",
    "        max_test_H_dict['rec']=H_rec\n",
    "        max_test_H_dict['f1_test']=H_f1_test\n",
    "        max_test_H_dict['f1_test_normal']=H_f1_test_normal\n",
    "        max_test_H_dict['f1_test_AF']=H_f1_test_AF\n",
    "        max_test_H_dict['f1_test_Other']=H_f1_test_Other\n",
    "        max_test_H_dict['acc']=H_acc\n",
    "        \n",
    "    print(f1_val_all)\n",
    "        \n",
    "print('max_epoch=',max_epoch)\n",
    "print('max_val_f1_all=',max_val_f1_all)\n",
    "        \n",
    "print (max_val_f1_all,max_epoch)\n",
    "print('_____________H_____________::')\n",
    "print(list(max_test_H_dict.values()))\n",
    "print('___________________________::')                                         \n",
    "\n",
    "\n",
    "\n",
    "# 写入csv\n",
    "\n",
    "header=[\n",
    "        'train\\\\test\\t',\n",
    "        'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "#         'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "\n",
    "        # round\n",
    "\n",
    "results=[\n",
    "        [\n",
    "        'H',\n",
    "        max_test_H_dict['auc'],\n",
    "        max_test_H_dict['pre'],\n",
    "        max_test_H_dict['rec'],\n",
    "        max_test_H_dict['f1_test'],\n",
    "        max_test_H_dict['f1_test_normal'],\n",
    "        max_test_H_dict['f1_test_AF'],\n",
    "        max_test_H_dict['f1_test_Other'],\n",
    "        max_test_H_dict['acc']+'\\t',\n",
    "         ],\n",
    "        ]\n",
    "\n",
    "with open('log/proposed_ABCLN_H_zishiying_600000_20240321_total_f1.csv','a',encoding='utf-8')as file_obj:\n",
    "    writer=csv.writer(file_obj)\n",
    "    writer.writerow(header)\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f666e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 更改加载的教师模型字典：\n",
    "# 加载5项不同的教师模型\n",
    "# 开始训练流程\n",
    "Teacher_queue=TeacherQueue(max_size=CRL_queue_len)\n",
    "student_model=generate_model(base_model=base_model_student,input_channels=2, num_classes=class_num,DG_method=DG_method_student,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model=generate_model(base_model=base_model_teacher,input_channels=12, num_classes=class_num,DG_method=DG_method_teacher,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model.load_state_dict(torch.load('/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/1711091430_ABCLN_H_proposed_cnnAg_zong_f1.pth'))\n",
    "teacher_model.eval()\n",
    "print('ok')\n",
    "optimizer=generate_optimizer(base_optimizer=base_optimizer,model_parameters=student_model.parameters(),SAM_flag=False,weight_decay=1e-4)\n",
    "\n",
    "mdst_path='/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/'\n",
    "mdst_name=str(int(time.time()))+'_ABCLN_H_zishiying_proposed_xuesheng_cnnAg_zong_f1.pth'\n",
    "\n",
    "max_val_f1_all=0\n",
    "max_epoch=0\n",
    "max_test_H_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "# max_test_R_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    print('_____________________________________')\n",
    "    print(' ')\n",
    "    time.sleep(0.1)\n",
    "    train_student_one_epoch(epoch,warm_epoch,total_epoch,student_model,teacher_model, \\\n",
    "                            max_lr,min_lr,loss_fn_clf,loss_fn_tslb,optimizer, \\\n",
    "                            feature_contrastive_loss_fn, \\\n",
    "                            Teacher_queue,base_optimizer, \\\n",
    "                            train_datasloader,device,GRL_loss_beta,GAN_aug='0211',GAN_model=cls_model)\n",
    "    \n",
    "    auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_A, device)\n",
    "    print(auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_B, device)\n",
    "    print(auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_C, device)\n",
    "    print(auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_N, device)\n",
    "    print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_L, device)\n",
    "    print(auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc)\n",
    "#     auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           val_datasloader_N, device)\n",
    "#     print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "                                         \n",
    "    H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          test_H_datasloader, device)\n",
    "    print(H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc)\n",
    "\n",
    "#     R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           test_R_datasloader, device)\n",
    "#     print(R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc)\n",
    "    \n",
    "    f1_val_all=int(f1_val_A)+int(f1_val_B)+int(f1_val_C)+int(f1_val_N)+int(f1_val_L)\n",
    "    \n",
    "    if int(f1_val_all)>max_val_f1_all:\n",
    "        max_val_f1_all=int(f1_val_all)\n",
    "        max_epoch=epoch\n",
    "        max_model_dict=student_model.state_dict()\n",
    "        # 指定你想要保存模型参数的路径  \n",
    "        torch.save(max_model_dict, os.path.join(mdst_path, mdst_name))\n",
    "        # 使用torch.save()保存模型参数  \n",
    "        \n",
    "        max_test_H_dict['auc']=H_auc\n",
    "        max_test_H_dict['pre']=H_pre\n",
    "        max_test_H_dict['rec']=H_rec\n",
    "        max_test_H_dict['f1_test']=H_f1_test\n",
    "        max_test_H_dict['f1_test_normal']=H_f1_test_normal\n",
    "        max_test_H_dict['f1_test_AF']=H_f1_test_AF\n",
    "        max_test_H_dict['f1_test_Other']=H_f1_test_Other\n",
    "        max_test_H_dict['acc']=H_acc\n",
    "        \n",
    "    print(f1_val_all)\n",
    "        \n",
    "print('max_epoch=',max_epoch)\n",
    "print('max_val_f1_all=',max_val_f1_all)\n",
    "        \n",
    "print (max_val_f1_all,max_epoch)\n",
    "print('_____________H_____________::')\n",
    "print(list(max_test_H_dict.values()))\n",
    "print('___________________________::')                                         \n",
    "\n",
    "\n",
    "\n",
    "# 写入csv\n",
    "\n",
    "header=[\n",
    "        'train\\\\test\\t',\n",
    "        'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "#         'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "\n",
    "        # round\n",
    "\n",
    "results=[\n",
    "        [\n",
    "        'H',\n",
    "        max_test_H_dict['auc'],\n",
    "        max_test_H_dict['pre'],\n",
    "        max_test_H_dict['rec'],\n",
    "        max_test_H_dict['f1_test'],\n",
    "        max_test_H_dict['f1_test_normal'],\n",
    "        max_test_H_dict['f1_test_AF'],\n",
    "        max_test_H_dict['f1_test_Other'],\n",
    "        max_test_H_dict['acc']+'\\t',\n",
    "         ],\n",
    "        ]\n",
    "\n",
    "with open('log/proposed_ABCLN_H_zishiying_600000_20240321_total_f1.csv','a',encoding='utf-8')as file_obj:\n",
    "    writer=csv.writer(file_obj)\n",
    "    writer.writerow(header)\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb9a3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 更改加载的教师模型字典：\n",
    "# 加载5项不同的教师模型\n",
    "# 开始训练流程\n",
    "Teacher_queue=TeacherQueue(max_size=CRL_queue_len)\n",
    "student_model=generate_model(base_model=base_model_student,input_channels=2, num_classes=class_num,DG_method=DG_method_student,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model=generate_model(base_model=base_model_teacher,input_channels=12, num_classes=class_num,DG_method=DG_method_teacher,domain_classes=len(train_data_list),distill=distill).to(device)\n",
    "teacher_model.load_state_dict(torch.load('/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/1711089958_ABCLN_H_proposed_cnnAg_zong_f1.pth'))\n",
    "teacher_model.eval()\n",
    "print('ok')\n",
    "optimizer=generate_optimizer(base_optimizer=base_optimizer,model_parameters=student_model.parameters(),SAM_flag=False,weight_decay=1e-4)\n",
    "\n",
    "mdst_path='/home/yangli/AF_DG_OODG/save_model_states/ABCLN_H_/'\n",
    "mdst_name=str(int(time.time()))+'_ABCLN_H_zishiying_proposed_xuesheng_cnnAg_zong_f1.pth'\n",
    "\n",
    "max_val_f1_all=0\n",
    "max_epoch=0\n",
    "max_test_H_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "# max_test_R_dict={'auc':0,'pre':0,'rec':0,'f1_test':0,'f1_test_normal':0,'f1_test_AF':0,'f1_test_Other':0,'acc':0}\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    print('_____________________________________')\n",
    "    print(' ')\n",
    "    time.sleep(0.1)\n",
    "    train_student_one_epoch(epoch,warm_epoch,total_epoch,student_model,teacher_model, \\\n",
    "                            max_lr,min_lr,loss_fn_clf,loss_fn_tslb,optimizer, \\\n",
    "                            feature_contrastive_loss_fn, \\\n",
    "                            Teacher_queue,base_optimizer, \\\n",
    "                            train_datasloader,device,GRL_loss_beta,GAN_aug='0211',GAN_model=cls_model)\n",
    "    \n",
    "    auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_A, device)\n",
    "    print(auc,pre,rec,f1_val_A,f1_val_normal,f1_val_AF_A,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_B, device)\n",
    "    print(auc,pre,rec,f1_val_B,f1_val_normal,f1_val_AF_B,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_C, device)\n",
    "    print(auc,pre,rec,f1_val_C,f1_val_normal,f1_val_AF_C,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_N, device)\n",
    "    print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "    auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          val_datasloader_L, device)\n",
    "    print(auc,pre,rec,f1_val_L,f1_val_normal,f1_val_AF_L,f1_val_Other,acc)\n",
    "#     auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc=val_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           val_datasloader_N, device)\n",
    "#     print(auc,pre,rec,f1_val_N,f1_val_normal,f1_val_AF_N,f1_val_Other,acc)\n",
    "                                         \n",
    "    H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "                          test_H_datasloader, device)\n",
    "    print(H_auc,H_pre,H_rec,H_f1_test,H_f1_test_normal,H_f1_test_AF,H_f1_test_Other,H_acc)\n",
    "\n",
    "#     R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc=test_student_one_epoch(epoch,warm_epoch, total_epoch, student_model,loss_fn_clf, \\\n",
    "#                           test_R_datasloader, device)\n",
    "#     print(R_auc,R_pre,R_rec,R_f1_test,R_f1_test_normal,R_f1_test_AF,R_f1_test_Other,R_acc)\n",
    "    \n",
    "    f1_val_all=int(f1_val_A)+int(f1_val_B)+int(f1_val_C)+int(f1_val_N)+int(f1_val_L)\n",
    "    \n",
    "    if int(f1_val_all)>max_val_f1_all:\n",
    "        max_val_f1_all=int(f1_val_all)\n",
    "        max_epoch=epoch\n",
    "        max_model_dict=student_model.state_dict()\n",
    "        # 指定你想要保存模型参数的路径  \n",
    "        torch.save(max_model_dict, os.path.join(mdst_path, mdst_name))\n",
    "        # 使用torch.save()保存模型参数  \n",
    "        \n",
    "        max_test_H_dict['auc']=H_auc\n",
    "        max_test_H_dict['pre']=H_pre\n",
    "        max_test_H_dict['rec']=H_rec\n",
    "        max_test_H_dict['f1_test']=H_f1_test\n",
    "        max_test_H_dict['f1_test_normal']=H_f1_test_normal\n",
    "        max_test_H_dict['f1_test_AF']=H_f1_test_AF\n",
    "        max_test_H_dict['f1_test_Other']=H_f1_test_Other\n",
    "        max_test_H_dict['acc']=H_acc\n",
    "        \n",
    "    print(f1_val_all)\n",
    "        \n",
    "print('max_epoch=',max_epoch)\n",
    "print('max_val_f1_all=',max_val_f1_all)\n",
    "        \n",
    "print (max_val_f1_all,max_epoch)\n",
    "print('_____________H_____________::')\n",
    "print(list(max_test_H_dict.values()))\n",
    "print('___________________________::')                                         \n",
    "\n",
    "\n",
    "\n",
    "# 写入csv\n",
    "\n",
    "header=[\n",
    "        'train\\\\test\\t',\n",
    "        'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "#         'auc','pre','rec','f1','normal-f1','AF-f1','other-f1','acc\\t',]\n",
    "\n",
    "        # round\n",
    "\n",
    "results=[\n",
    "        [\n",
    "        'H',\n",
    "        max_test_H_dict['auc'],\n",
    "        max_test_H_dict['pre'],\n",
    "        max_test_H_dict['rec'],\n",
    "        max_test_H_dict['f1_test'],\n",
    "        max_test_H_dict['f1_test_normal'],\n",
    "        max_test_H_dict['f1_test_AF'],\n",
    "        max_test_H_dict['f1_test_Other'],\n",
    "        max_test_H_dict['acc']+'\\t',\n",
    "         ],\n",
    "        ]\n",
    "\n",
    "with open('log/proposed_ABCLN_H_zishiying_600000_20240321_total_f1.csv','a',encoding='utf-8')as file_obj:\n",
    "    writer=csv.writer(file_obj)\n",
    "    writer.writerow(header)\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
