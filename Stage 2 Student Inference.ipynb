{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c922c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mvclab-gpuserver-57        \u001b[m  Tue Dec 16 01:31:44 2025  \u001b[1m\u001b[30m550.78\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 28'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  324\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 31'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  324\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 26'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  324\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 26'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  324\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 27'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  324\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 29'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  324\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[6]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 26'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  324\u001b[m / \u001b[33m24576\u001b[m MB |\n",
      "\u001b[36m[7]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 27'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  324\u001b[m / \u001b[33m24576\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "812c3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库引入\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] ='1'\n",
    "device='cuda:0'\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import wfdb\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics\n",
    "from scipy import signal\n",
    "import torch.nn.init as init\n",
    "from collections import deque  \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import defaultdict\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "from models.DG_models import generate_model\n",
    "from datasets import train_loader,val_loader,test_loader\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "\n",
    "loss_fn_clf=nn.CrossEntropyLoss()\n",
    "train_val_test_split=[0.8,0.2,1] # 1 for inter\n",
    "batch_size=128\n",
    "class_num=3\n",
    "smote=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3231eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data L\n",
      "train data size : 16824\n",
      "train label class :  [13511.  1158.  2155. 16824.     0.     0.     0.     0.]\n",
      "train data B\n",
      "train data size : 5501\n",
      "train label class :  [ 742.  859. 3900.    0. 5501.    0.    0.    0.]\n",
      "train data C\n",
      "train data size : 24194\n",
      "train label class :  [ 6719.  2039. 15436.     0.     0. 24194.     0.     0.]\n",
      "train data A\n",
      "train data size : 8274\n",
      "train label class :  [1393.  480. 6401.    0.    0.    0. 8274.    0.]\n",
      "train data N\n",
      "train data size : 8516\n",
      "train label class :  [1462. 1769. 5285.    0.    0.    0.    0. 8516.]\n",
      "total train data shape =  (63309, 1000, 1)\n",
      "total train label shape =  (63309, 8)\n",
      "total train data class =  [23827.  6305. 33177. 16824.  5501. 24194.  8274.  8516.]\n",
      "val data A\n",
      "val data size : 2069\n",
      "val label class :  [ 359.   90. 1620.]\n",
      "total val data shape =  (2069, 1000, 1)\n",
      "total val label shape =  (2069, 3)\n",
      "total val data class =  [ 359.   90. 1620.]\n",
      "val data B\n",
      "val data size : 1376\n",
      "val label class :  [176. 239. 961.]\n",
      "total val data shape =  (1376, 1000, 1)\n",
      "total val label shape =  (1376, 3)\n",
      "total val data class =  [176. 239. 961.]\n",
      "val data C\n",
      "val data size : 6049\n",
      "val label class :  [1485.  584. 3980.]\n",
      "total val data shape =  (6049, 1000, 1)\n",
      "total val label shape =  (6049, 3)\n",
      "total val data class =  [1485.  584. 3980.]\n",
      "val data L\n",
      "val data size : 4206\n",
      "val label class :  [3237.  356.  613.]\n",
      "total val data shape =  (4206, 1000, 1)\n",
      "total val label shape =  (4206, 3)\n",
      "total val data class =  [3237.  356.  613.]\n",
      "val data N\n",
      "val data size : 2130\n",
      "val label class :  [ 364.  456. 1310.]\n",
      "total val data shape =  (2130, 1000, 1)\n",
      "total val label shape =  (2130, 3)\n",
      "total val data class =  [ 364.  456. 1310.]\n",
      "test data H\n",
      "test data size : 25770\n",
      "test label class :  [13905.   675. 11190.]\n",
      "total test data shape =  (25770, 1000, 1)\n",
      "total test label shape =  (25770, 3)\n",
      "total test data class =  [13905.   675. 11190.]\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_data_list=['L','B','C','A','N']\n",
    "\n",
    "train_datasloader_instance=train_loader(dataname=train_data_list,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,smote=smote,maple=False,class_num=class_num)\n",
    "train_datasloader=train_datasloader_instance.loader()\n",
    "\n",
    "val_data_list_A=['A']\n",
    "val_datasloader_A=val_loader(dataname=val_data_list_A,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "\n",
    "val_data_list_B=['B']\n",
    "val_datasloader_B=val_loader(dataname=val_data_list_B,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "\n",
    "val_data_list_C=['C']\n",
    "val_datasloader_C=val_loader(dataname=val_data_list_C,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "\n",
    "# val_data_list_H=['H']\n",
    "# val_datasloader_H=val_loader(dataname=val_data_list_H,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "\n",
    "val_data_list_L=['L']\n",
    "val_datasloader_L=val_loader(dataname=val_data_list_L,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "\n",
    "val_data_list_N=['N']\n",
    "val_datasloader_N=val_loader(dataname=val_data_list_N,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "\n",
    "test_H_data_list=['H']\n",
    "test_H_datasloader=test_loader(dataname=test_H_data_list,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "\n",
    "\n",
    "# test_T_data_list=['T_3cls']\n",
    "# test_T_datasloader=test_loader(dataname=test_T_data_list,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n",
    "# test_R_data_list=['R_3cls']\n",
    "# test_R_datasloader=test_loader(dataname=test_R_data_list,train_val_test_split=train_val_test_split,bs=batch_size,num_workers=2,class_num=class_num).loader()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab2f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成模型定义，包括两个encoder和一个decoder\n",
    "\n",
    "class AdaptiveInstanceNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(AdaptiveInstanceNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # weight and bias are dynamically assigned\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        # just dummy buffers, not used\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.weight is not None and self.bias is not None, \"Please assign weight and bias before calling AdaIN!\"\n",
    "        b, c = x.size(0), x.size(1)\n",
    "        running_mean = self.running_mean.repeat(b)\n",
    "        running_var = self.running_var.repeat(b)\n",
    "\n",
    "        # Apply instance norm\n",
    "        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n",
    "        out = F.batch_norm(\n",
    "            x_reshaped, running_mean, running_var, self.weight, self.bias,\n",
    "            True, self.momentum, self.eps)\n",
    "\n",
    "        return out.view(b, c, *x.size()[2:])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + str(self.num_features) + ')'\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # dec\n",
    "        self.conv1=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn1=AdaptiveInstanceNorm2d(64)\n",
    "        self.ac1=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv2=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2=AdaptiveInstanceNorm2d(64)\n",
    "        self.ac2=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv3=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn3=AdaptiveInstanceNorm2d(64)\n",
    "        self.ac3=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv4=nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn4=AdaptiveInstanceNorm2d(32)\n",
    "        self.ac4=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv5=nn.Conv1d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn5=AdaptiveInstanceNorm2d(32)\n",
    "        self.ac5=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv6=nn.Conv1d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn6=AdaptiveInstanceNorm2d(16)\n",
    "        self.ac6=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv7=nn.Conv1d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn7=AdaptiveInstanceNorm2d(16)\n",
    "        self.ac7=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv8=nn.Conv1d(16, 8, kernel_size=3, padding=1)\n",
    "        self.bn8=AdaptiveInstanceNorm2d(8)\n",
    "        self.ac8=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv9=nn.Conv1d(8, 8, kernel_size=3, padding=1)\n",
    "        self.ac9=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv10=nn.Conv1d(8, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x=self.ac1(x+self.bn1(self.conv1(x)))\n",
    "        x=self.ac2(x+self.bn2(self.conv2(x)))\n",
    "        x=self.ac3(x+self.bn3(self.conv3(x)))\n",
    "        x=self.ac4(self.bn4(self.conv4(x)))\n",
    "        x=self.ac5(self.bn5(self.conv5(x)))\n",
    "        x=self.ac6(self.bn6(self.conv6(x)))\n",
    "        x=self.ac7(self.bn7(self.conv7(x)))\n",
    "        x=self.ac8(self.bn8(self.conv8(x)))\n",
    "        x=self.conv10(self.ac9(self.conv9(x)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ECG_gen(nn.Module):  \n",
    "    def __init__(self,):  \n",
    "        super(ECG_gen, self).__init__()  \n",
    "        \n",
    "        # cont_enc\n",
    "        \n",
    "        self.conv1=nn.Conv1d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1=nn.InstanceNorm1d(8)\n",
    "        self.ac1=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv2=nn.Conv1d(8, 8, kernel_size=3, padding=1)\n",
    "        self.bn2=nn.InstanceNorm1d(8)\n",
    "        self.ac2=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv3=nn.Conv1d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn3=nn.InstanceNorm1d(16)\n",
    "        self.ac3=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv4=nn.Conv1d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn4=nn.InstanceNorm1d(16)\n",
    "        self.ac4=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv5=nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn5=nn.InstanceNorm1d(32)\n",
    "        self.ac5=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv6=nn.Conv1d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn6=nn.InstanceNorm1d(32)\n",
    "        self.ac6=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        self.conv7=nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn7=nn.InstanceNorm1d(64)\n",
    "        self.ac7=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv8=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn8=nn.InstanceNorm1d(64)\n",
    "        self.ac8=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv9=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn9=nn.InstanceNorm1d(64)\n",
    "        self.ac9=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv10=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn10=nn.InstanceNorm1d(64)\n",
    "        self.ac10=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv11=nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn11=nn.InstanceNorm1d(64)\n",
    "        self.ac11=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        \n",
    "        # style_enc\n",
    "        \n",
    "        self.conv_sty_1=nn.Conv1d(1, 8, kernel_size=3, padding=1)\n",
    "        self.ac_sty_1=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_2=nn.Conv1d(8, 16, kernel_size=3, padding=1)\n",
    "        self.ac_sty_2=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_3=nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.ac_sty_3=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_4=nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.ac_sty_4=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_5=nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.ac_sty_5=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.conv_sty_6=nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.ac_sty_6=nn.LeakyReLU(0.01, inplace=True)  # Replace ReLU with LeakyReLU\n",
    "        self.pool_sty=nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc_sty=nn.Conv1d(256, 8, kernel_size=1, padding=0)\n",
    "        # bs,8,1\n",
    "        \n",
    "        self.dec=Decoder()\n",
    "        \n",
    "        self.mlp_1=nn.Linear(8, 256, bias=True)\n",
    "        self.mlp_2=nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.mlp_3=nn.Linear(256, 256, bias=True)\n",
    "        self.mlp_4=nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.mlp_5=nn.Linear(256, self.get_num_adain_params(self.dec), bias=True)\n",
    "        \n",
    "        \n",
    "    def encode(self, x):  # bs,1000,1  x1是特征，x2是风格\n",
    "        \n",
    "        x=x.transpose(-1,-2) # bs,1,1000\n",
    "        \n",
    "        xc=self.conv1(x)\n",
    "        xc=self.bn1(xc)\n",
    "        xc=self.ac1(xc)\n",
    "        xc=self.ac2(self.bn2(self.conv2(xc)))\n",
    "        xc=self.ac2(self.bn2(self.conv2(xc)))\n",
    "        xc=self.ac3(self.bn3(self.conv3(xc)))\n",
    "        xc=self.ac4(self.bn4(self.conv4(xc)))\n",
    "        xc=self.ac5(self.bn5(self.conv5(xc)))\n",
    "        xc=self.ac6(self.bn6(self.conv6(xc)))\n",
    "        xc=self.ac7(self.bn7(self.conv7(xc)))\n",
    "        xc=self.ac8(self.bn8(self.conv8(xc)))\n",
    "        xc=self.ac9(xc+self.bn9(self.conv9(xc)))\n",
    "        xc=self.ac10(xc+self.bn10(self.conv10(xc)))\n",
    "        content=self.ac11(xc+self.bn11(self.conv11(xc)))   # bs,64,1000\n",
    "\n",
    "        \n",
    "        \n",
    "        xs=self.conv_sty_1(x)\n",
    "        xs=self.ac_sty_1(xs)\n",
    "        xs=self.conv_sty_2(xs)\n",
    "        xs=self.ac_sty_2(xs)\n",
    "        xs=self.conv_sty_3(xs)\n",
    "        xs=self.ac_sty_3(xs)\n",
    "        xs=self.conv_sty_4(xs)\n",
    "        xs=self.ac_sty_4(xs)\n",
    "        xs=self.conv_sty_5(xs)\n",
    "        xs=self.ac_sty_5(xs)\n",
    "        xs=self.conv_sty_6(xs)\n",
    "        xs=self.ac_sty_6(xs)\n",
    "        xs=self.pool_sty(xs)\n",
    "        style=self.fc_sty(xs)\n",
    "        \n",
    "        return content, style\n",
    "    \n",
    "    def decode(self,content, style):  # x1逆向操作，x2应用到自适应归一化层\n",
    "        \n",
    "        adain_params = self.mlp_1(style.view(style.size(0), -1))\n",
    "        adain_params = self.mlp_2(adain_params)\n",
    "        adain_params = self.mlp_3(adain_params)\n",
    "        adain_params = self.mlp_4(adain_params)\n",
    "        adain_params = self.mlp_5(adain_params)\n",
    "        self.assign_adain_params(adain_params, self.dec)\n",
    "        x=self.dec(content)\n",
    "        x=x.transpose(-1,-2)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def assign_adain_params(self, adain_params, model):\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                mean = adain_params[:, :m.num_features]\n",
    "                std = adain_params[:, m.num_features:2*m.num_features]\n",
    "                m.bias = mean.contiguous().view(-1)\n",
    "                m.weight = std.contiguous().view(-1)\n",
    "                if adain_params.size(1) > 2*m.num_features:\n",
    "                    adain_params = adain_params[:, 2*m.num_features:]\n",
    "        \n",
    "        \n",
    "    def get_num_adain_params(self,model):\n",
    "        num_adain_params = 0\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                num_adain_params += 2*m.num_features\n",
    "        return num_adain_params\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # reconstruct an image\n",
    "        content, style_fake = self.encode(images)\n",
    "        images_recon = self.decode(content, style_fake)\n",
    "        return images_recon\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc0d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型的流程定义\n",
    "\n",
    "def load_munit_model(model_path):\n",
    "\n",
    "    return MUNITModelOfNatVar(model_path).to(device)\n",
    "\n",
    "\n",
    "\n",
    "class MUNITModelOfNatVar(nn.Module):\n",
    "    def __init__(self, fname: str):\n",
    "\n",
    "        super(MUNITModelOfNatVar, self).__init__()\n",
    "\n",
    "        self._fname = fname\n",
    "        self._gen_A, self._gen_B = self.__load()\n",
    "        self.delta_dim = 8\n",
    "\n",
    "    def forward(self, x, delta):\n",
    "\n",
    "        orig_content, _ = self._gen_A.encode(x)\n",
    "        orig_content = orig_content.clone().detach().requires_grad_(False)\n",
    "        x_out = self._gen_B.decode(orig_content, delta)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def __load(self):\n",
    "        \"\"\"Load MUNIT model from file.\"\"\"\n",
    "\n",
    "        def load_munit(fname, letter):\n",
    "            gen = ECG_gen()\n",
    "            gen.load_state_dict(torch.load(fname)[letter])\n",
    "            return gen.eval()\n",
    "\n",
    "        gen_A = load_munit(self._fname, 'a')\n",
    "        gen_B = load_munit(self._fname, 'b')\n",
    "\n",
    "        return gen_A, gen_B     # original order\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad69696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算法总模型定义\n",
    "\n",
    "\n",
    "class Algorithm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(Algorithm, self).__init__()\n",
    "\n",
    "    def update(self, in_data_, targets_, unlabeled=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ERM(Algorithm):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super(ERM, self).__init__()\n",
    "\n",
    "\n",
    "        self.network = generate_model(base_model='cnn_Ag',input_channels=1, num_classes=3,DG_method=None,domain_classes=6,distill=False)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(),lr=1e-3,weight_decay=0)\n",
    "\n",
    "    def update(self, in_data_, targets_, unlabeled=None):\n",
    "        \n",
    "        loss = F.cross_entropy(self.predict(in_data_), targets_)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return {'loss': loss.item()}\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class MBDG_Base(ERM):\n",
    "    def __init__(self, ):\n",
    "        super(MBDG_Base, self).__init__()\n",
    "    \n",
    "        # self.G = load_munit_model('AF_DG_OODG/GAN/gen_00600000.pt')\n",
    "        self.G = load_munit_model('AF_DG_OODG/GANMBDG/gen_00600000.pt')\n",
    "\n",
    "    @staticmethod\n",
    "    def kl_div(dist1, dist2):\n",
    "        return F.kl_div(torch.log(dist2), dist1, reduction='batchmean')\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_images(self, images):\n",
    "        delta = torch.randn(images.size(0), self.G.delta_dim).to(device).requires_grad_(False)\n",
    "        return self.G(images, delta)\n",
    "\n",
    "    def calc_dist_reg(self, x, clean_output):\n",
    "        mb_images = self.generate_images(x)\n",
    "        mb_output = F.softmax(self.predict(mb_images), dim=1)\n",
    "        return self.kl_div(F.softmax(clean_output, dim=1), mb_output)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return x if x > 0 else torch.tensor(0).to(device)\n",
    "\n",
    "class MBDG(MBDG_Base):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super(MBDG, self).__init__()\n",
    "        self.dual_var = torch.tensor(1.0).to(device).requires_grad_(False)\n",
    "\n",
    "    def update(self, in_data_, targets_, unlabeled=None):\n",
    "\n",
    "        clean_output = self.predict(in_data_)\n",
    "        clean_loss = F.cross_entropy(clean_output, targets_)\n",
    "        dist_reg = self.calc_dist_reg(in_data_, clean_output)\n",
    "\n",
    "        loss = clean_loss + self.dual_var * dist_reg\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        const_unsat = dist_reg.detach() - 0.025\n",
    "        self.dual_var = self.relu(self.dual_var + 0.05 * const_unsat)\n",
    "\n",
    "        return {'loss': loss.item(), 'dist_reg': dist_reg.item(), 'dual_var': self.dual_var.item()}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929a1065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaojianlin/miniconda3/envs/clkg_ecp/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/xiaojianlin/miniconda3/envs/clkg_ecp/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model=cnn_Ag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3592912/2002937344.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  gen.load_state_dict(torch.load(fname)[letter])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params of model: 11.46 M\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "from deepspeed.accelerator import get_accelerator\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "with get_accelerator().device(0):\n",
    "    cls_model=MBDG().to(device)\n",
    "    cls_model.eval()\n",
    "\n",
    "    val_datasloader_A_iter = iter(val_datasloader_A)\n",
    "    in_data_, targets_ = next(val_datasloader_A_iter)\n",
    "    input_shape = tuple(in_data_.shape)\n",
    "    \n",
    "    flops, macs, params = get_model_profile(model=cls_model, # model\n",
    "                                    input_shape=input_shape, # input shape to the model. If specified, the model takes a tensor with this shape as the only positional argument.\n",
    "                                    args=None, # list of positional arguments to the model.\n",
    "                                    kwargs=None, # dictionary of keyword arguments to the model.\n",
    "                                    print_profile=False, # prints the model graph with the measured profile attached to each module\n",
    "                                    detailed=True, # print the detailed profile\n",
    "                                    module_depth=-1, # depth into the nested modules, with -1 being the inner most modules\n",
    "                                    top_modules=1, # the number of top modules to print aggregated profile\n",
    "                                    warm_up=10, # the number of warm-ups before measuring the time of each module\n",
    "                                    as_string=True, # print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)\n",
    "                                    output_file=None, # path to the output file. If None, the profiler prints to stdout.\n",
    "                                    ignore_modules=None) # the list of modules to ignore in the profiling\n",
    "    print(f'params of model: {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1141cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clkg_ecp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
